[
  {
    "objectID": "posts/first-post/index.html",
    "href": "posts/first-post/index.html",
    "title": "First Post",
    "section": "",
    "text": "Instead of porting my blog, I decided that I would start from scratch so I could keep the posts that I wanted, and fix the ones that I would keep. I have learned a lot since the start of my original blog, and I thought it might be best to make note of that. Instead of removing less efficient approaches that my old posts might have, this will be a good chance to take advantage of the features that Quarto provide. I’ll give an example in my next post.\nGiven how much easier it will be to post, my goal will be to have a new blog post at least once a month. From this post it will not be too much of a challenge since I have a lot of drafts saved, however, my goal is for sustainability in the future.\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog-becausejustyn",
    "section": "",
    "text": "r\n\n\nnfl\n\n\nelo\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nnfl\n\n\nunsupervised learning\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsport\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nggplot\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nggplot\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/virtual-environments/index.html",
    "href": "posts/virtual-environments/index.html",
    "title": "Virtual Environments",
    "section": "",
    "text": "My supervisor suggests that I use Google Collab for any code that takes more than 3 minutes to run.\nOnce I started using Python more and required libraries beyond Numpy and Pandas, I began to see the challenge of dealing with multiple conflicts. Additionally, there was the challenge of not being able to do something because one library required a particular version of Python.\n\nvenv\nSetting up a venv is quite simple. For mac, you simply would type the following into your terminal\n\n\nCode\npython3 -m venv env\n\n\nWhich would create a new enviornment called venv. Something worth noting is that loading the kernel will be much quick when you create a venv in your project.\n\n\nOne of the nice things about Quarto is that it forces you to become more comfortable with command line. If you do not want to learn command line terminology, you can use RStudio, which has user friendly options, however, I think in the long run being more comfortable with some of the terms will help you out.\nTo activate the environment, you simply would type\n\n\nCode\nsource env/bin/activate\n\n\nInstalling packages for your venv is quite simple.\n\n\nCode\npython3 -m pip install numpy jupyter matplotlib pandas plotly\n\n\nSay you want your environment to be reproducible, you will want to create a requirements document so someone can run requirements.txt and install the libraries from there.\n\n\nCode\npython3 -m pip freeze > requirements.txt\n\n\nWhich is quite simple for any user on a different machine. All you would have to do is\n\n\nCode\npython3 -m pip install -r requirements.txt\n\n\n\n\nrenv\nrenv is the R version of venv. Given the major changes between key versions, it is a good practice to use renv when working on something that other users might also work on.\n\n\nCode\ninstall.packages(\"renv\", repos='http://cran.us.r-project.org')\nrenv::init()\n\n\nFrom my understanding, if you do this, you will not need to set up a venv like above. I use Visual Code for this project, which automatically loads the enviornment in your local folder so that makes it easier.\n\n\nCode\nrenv::use_python()"
  },
  {
    "objectID": "posts/colouring-text-ggplot/index.html",
    "href": "posts/colouring-text-ggplot/index.html",
    "title": "Colouring Text in ggplot2",
    "section": "",
    "text": "I pr\n\n\nCode\nmtcars %>%\n  ggplot(aes(\n    x = wt, \n    y = mpg, \n    colour = factor(cyl)\n  )) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Set2\") +\n  labs(\n    x = \"<span style = 'color:#93C1DE'>**wt**</span>\",\n    y = \"<span style = 'color:#2ca25f'>**mpg**</span>\",\n    title = \"Lower <span style='color:#93C1DE'>**wt**</span> tends to increase the amount of <span style='color:#2ca25f'>**mpg**</span>\",\n    colour = \"\"\n  ) +\n  theme(\n    plot.title = element_markdown(),\n    axis.title.x = element_markdown(),\n    axis.title.y = element_markdown()\n  )\n\n\n\n\n\nColouring one of interest\n\n\nCode\niris %>%\n  group_by(Species) %>%\n  summarise(mean_petal_width = mean(Petal.Width), .groups = \"drop\") %>%\n  mutate(\n    colour = c(\"lightgray\", \"lightgray\", \"#0072B2\"), #009E73\n    name = glue(\"<i style='color:{colour}'>{Species}</i>\"),\n    name = fct_reorder(name, mean_petal_width)\n  ) %>%\n  ggplot(aes(\n    x = name,\n    y = mean_petal_width,\n    fill = colour\n  )) +\n  geom_col() +\n  hrbrthemes::theme_ipsum() +\n  scale_fill_identity() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"<span style = 'color: #43a2ca;'>Virginica irises</span> have the largest average sepal width\"\n  ) +\n  theme(\n    plot.title = element_markdown(),\n    axis.text.x = element_markdown(),\n    legend.position = \"none\"\n  )\n\n\n\n\n\nHaving them all coloured\n\n\nCode\niris %>%\n  group_by(Species) %>%\n  summarise(mean_petal_width = mean(Petal.Width), .groups = \"drop\") %>%\n  mutate(\n    colour = c(\"#91529e\", \"#009E73\", \"#0072B2\"), #009E73\n    name = glue(\"<i style='color:{colour}'>{Species}</i>\"),\n    name = fct_reorder(name, mean_petal_width)\n  ) %>%\n  ggplot(aes(\n    x = name,\n    y = mean_petal_width,\n    fill = colour\n  )) +\n  geom_col() +\n  hrbrthemes::theme_ipsum() +\n  scale_fill_identity() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"<span style = 'color: #43a2ca;'>Virginica irises</span> have the largest average sepal width\"\n  ) +\n  theme(\n    plot.title = element_markdown(),\n    axis.text.x = element_markdown(),\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/gambling/index.html",
    "href": "posts/gambling/index.html",
    "title": "Gambling",
    "section": "",
    "text": "As a sports fan I have always enjoyed looking at win probability. I was amazed the day that I discovered head to head odds and a spread can be roughly converted to a teams expected win probability. There are three main types of odds used in sport:\nDecimal Odds: 3.5 or 3.5:1\nFractional Odds: 7/2\nMoneyline Odds: +350\nI find moneyline odds not very intuitive, so I am constantly having to search for ways to convert them. The logic for them is\nI have never liked this explaination because not only does it create an expectation on how much money one should bet, it also is not intuitive since you need to remove the initial money invested in the return since that was not money won.\nI used to gamble causally before (my biggest bet was $5), however, as I began to understand the odds and expected return better, I began to gamble less and less often. I am going to display how to convert these odds from one type to another now."
  },
  {
    "objectID": "posts/gambling/index.html#calculating-these-via-r-or-python",
    "href": "posts/gambling/index.html#calculating-these-via-r-or-python",
    "title": "Gambling",
    "section": "Calculating these via R or Python",
    "text": "Calculating these via R or Python\nglue() in R is the same as using f strings in Python, however, I wanted to display it in base R since I did not import anything for Python.\n\n\nIf I were to use glue(), it would be done via glue::glue(\"Favourite odds: {fav_prob}\\n Underdog odds:{underdog_prob}\")\n\nRPython\n\n\n\n\nCode\nmoney_line <- function(fav, underdog, ndigits = 3) {\n  \n  fav_odds = (fav * -1)\n  prob1 = fav_odds / (fav_odds + underdog)\n  prob2 = underdog / (fav_odds + underdog)\n  fav_prob = prob1 |> round(ndigits)\n  underdog_prob = prob2 |> round(ndigits)\n  \n  cat(\"Favourite odds\", fav_prob, \"\\nUnderdog odds: \", underdog_prob)\n}\n\n\n\n\nCode\nmoney_line(-130, 110)\n\n\nFavourite odds 0.542 \nUnderdog odds:  0.458\n\n\n\n\n\n\nCode\ndef money_line(fav, underdog, ndigits = 3):\n    fav_odds = (fav * -1)\n    prob1 = fav_odds / (fav_odds + underdog)\n    prob2 = underdog / (fav_odds + underdog)\n    fav_prob = round(prob1, ndigits)\n    underdog_prob = round(prob2, ndigits)\n\n    print(f\"\\nFavourite odds: {fav_prob} \\nUnderdog odds:{underdog_prob}\")\n\n\n\n\nCode\nmoney_line(-130, 110)\n\n\n\nFavourite odds: 0.542 \nUnderdog odds:0.458\n\n\n\n\n\nThe problem with the above function is that it requires the input values to be in a specific order. If you want a more general approach that does not specify the order of the two values, you could do something like this.\n\nRPython\n\n\n\n\nCode\nmoney_line2 <- function(arg1, arg2, ndigits = 3) {\n  \n  fav = sort(c(arg1, arg2))[1]\n  underdog = sort(c(arg1, arg2))[2]    \n  fav_val = fav * -1\n  fav_prob = fav_val / (fav_val + underdog)\n  return(fav_prob |> round(ndigits))\n}\n\n\n\n\nCode\nprint(money_line2(-425, 351))\n\n\n[1] 0.548\n\n\nCode\nprint(money_line2(351, -425))\n\n\n[1] 0.548\n\n\n\n\n\n\nCode\ndef money_line2(arg1, arg2, ndigits = 3):\n\n    fav = sorted([arg1, arg2])[0]\n    underdog = sorted([arg1, arg2])[1]\n    # underdog will be negative, so this makes it positive\n    fav_val = fav * -1\n    fav_prob = fav_val / (fav_val + underdog)\n    return round(fav_prob, ndigits)\n\n\n\n\nCode\nprint(money_line2(-425, 351))\n\n\n0.548\n\n\nCode\nprint(money_line2(351, -425))\n\n\n0.548\n\n\n\n\n\nIt is worth noting that the above functions can be done in less lines, but I like defining variables on their own line to make the code a bit more readable. For example, say I wanted to create the sigmoid function\n\n\nI imported math for Python for the log function.\n\nRPython\n\n\n\n\nCode\nlogit <- function(p) { \n  out <- p / (1 - p) |> log()\n  return(out)\n}\n\nlogit <- function(p) { \n  return(log(p/(1 - p)))\n}\n\n(function (p) p/(1 - p) |>log()) (x)\n\n\n\n\n\n\nCode\nimport math\n\ndef logit(p):\n    out = math.log(p / (1 - p))\n    return(out)\n\ndef logit(p):\n    return(math.log(p / (1 - p)))\n\nx = lambda p: math.log(p / (1 - p))\n\n\n\n\n\nBoth functions in R or Python would return the same output. While readability may not seem that important for a simple function\n\n\n\n\n\n\nThe anonymous function versions for both would be: R: (function (p) p/(1 - p) |>log()) (x) Python: x = lambda p: math.log(p / (1 - p))"
  },
  {
    "objectID": "posts/tidy-pandas/index.html",
    "href": "posts/tidy-pandas/index.html",
    "title": "Dplyr vs Pandas",
    "section": "",
    "text": "Uni starts back up soon so I thought it would be a good idea to brush up on my python. I have avoided using python since R is much more user friendly between dplyr and ggplot2. Now that RStudio has python computability via reticulate there is not really a good reason to completely avoid python. I think it is always good practice to get more comfortable with different languages, because sometimes you will need to do a task that is only available using a specific tool. Similarly, there are times when a group project works best when everybody is able to use a similar language.\nAs tempting as it might be to do all my data wrangling via tidyverse, I have been practising using pandas. This post was entirely written in RStudio, however, the python code will run in a notebook alternative such as Jupyter or VS Code.\n\nInstalling Packages\n\nRPython\n\n\n\n\nCode\ninstall.packages(\"tidyverse\")\n\n\n\n\nCode\nx <- c(1, 3)\nprint(x[1])\n\n\n[1] 1\n\n\n\n\n\n\nCode\n!python3 -m pip install pandas seaborn numpy\n\n#You can also install these packages in the terminal \npython3 -m pip install pandas seaborn numpy\n\n\n\n\n\nLearning python was overwhelming because you use the terminal much more often than you need to when using R. Over time, I have began to appreciate using a virtual environment because you can easily run multiple versions of python, which makes it more practical when you are using a package that requires an older version.\n\n\nImporting Data, Loading Libraries\nI have hidden the output of the code, however, you can view it by clicking the dropdown menu. I did this primarily for myself so it was easier to scroll down, but I think it is also more practical since the output is not necessarily the focus of this post.\n\nRPython\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf <- diamonds\n\ndf %>% head()\n\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset('diamonds')\n\ndf.head()\n\n\n\n\n\n\n\nExample of Functions\n\n\nVerbsExample\n\n\n\n\n\ndplyr\npandas\n\n\n\n\nfilter() and slice()\nquery() and loc[], iloc[]\n\n\narrange()\nsort_values and sort_index()\n\n\nselect() and rename()\n__getitem__ and rename()\n\n\nselect()\nfilter()\n\n\ndistinct()\ndrop_duplicates()\n\n\nmutate()\nassign\n\n\nsummarise()\nagg\n\n\ngroup_by()\ngroupby()\n\n\nsample_n() and sample_frac()\nsample\n\n\n%>%\npipe[^1]\n\n\n\n\n\n\n\n\n\n\n\n\ndplyr\npandas\n\n\n\n\nfilter(df, col == 'val')\ndf.query('col == \"val\"')\n\n\narrange(df, col)\ndf.sort_values('val')\n\n\nrename(df, new_name = old_name)\ndf.rename(columns = {old_name = new_name})\n\n\nselect(df, col)\ndf.loc['val']\n\n\ndistinct(df, col, .keep_all = TRUE)\ndf[['val']].drop_duplicates()\n\n\nmutate(new_var = col - col2)\ndf.assign(new_var = df.col - df.col2)\n\n\nsummarise(mean = mean(col2), n = count(col1))\ndf.agg({\"col1\": \"count\", \"col2\", \"mean\"})\n\n\ngroup_by(df, col)\ndf.groupby('col')\n\n\n%>%\npipe[^1]\n\n\n\n\n\n\n\nOne of the confusing things are first is that there are many similar functions under different names. I personally find it easier to remember them by the way I write my code. For example, by only using <- as an assignment operator in R, I find it easier to treat the two languages differently.\n\n\nSelecting Columns\n\nRPython\n\n\n\n\nCode\nselect(df, color, cut)\n\n\n# A tibble: 53,940 × 2\n   color cut      \n   <ord> <ord>    \n 1 E     Ideal    \n 2 E     Premium  \n 3 E     Good     \n 4 I     Premium  \n 5 J     Good     \n 6 J     Very Good\n 7 I     Very Good\n 8 H     Very Good\n 9 E     Fair     \n10 H     Very Good\n# … with 53,930 more rows\n\n\n\n\n\n\nCode\ndf.filter(['color', 'cut'])\n\n#or\n#df[['color', 'cut']]\n\n\n\n\n\n\n\nIf we want to select a range of columns\n\nRPython\n\n\n\n\nCode\nselect(df, x:z)\n\n\n# A tibble: 53,940 × 3\n       x     y     z\n   <dbl> <dbl> <dbl>\n 1  3.95  3.98  2.43\n 2  3.89  3.84  2.31\n 3  4.05  4.07  2.31\n 4  4.2   4.23  2.63\n 5  4.34  4.35  2.75\n 6  3.94  3.96  2.48\n 7  3.95  3.98  2.47\n 8  4.07  4.11  2.53\n 9  3.87  3.78  2.49\n10  4     4.05  2.39\n# … with 53,930 more rows\n\n\n\n\n\n\nCode\ndf.loc[:, 'x':'z']\n\n\n\n\n\n\n\nIf we want to pipe it\n\nRPython\n\n\n\n\nCode\nselect(df, color, cut)\n\n\n# A tibble: 53,940 × 2\n   color cut      \n   <ord> <ord>    \n 1 E     Ideal    \n 2 E     Premium  \n 3 E     Good     \n 4 I     Premium  \n 5 J     Good     \n 6 J     Very Good\n 7 I     Very Good\n 8 H     Very Good\n 9 E     Fair     \n10 H     Very Good\n# … with 53,930 more rows\n\n\n\n\n\n\nCode\n(df\n.filter(['color', 'cut'])\n)\n\n\n\n\n\n\n\nIf we want to drop a certain column\n\nRPython\n\n\n\n\nCode\nselect(df, -(x:z))\n\n\n# A tibble: 53,940 × 7\n   carat cut       color clarity depth table price\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int>\n 1  0.23 Ideal     E     SI2      61.5    55   326\n 2  0.21 Premium   E     SI1      59.8    61   326\n 3  0.23 Good      E     VS1      56.9    65   327\n 4  0.29 Premium   I     VS2      62.4    58   334\n 5  0.31 Good      J     SI2      63.3    58   335\n 6  0.24 Very Good J     VVS2     62.8    57   336\n 7  0.24 Very Good I     VVS1     62.3    57   336\n 8  0.26 Very Good H     SI1      61.9    55   337\n 9  0.22 Fair      E     VS2      65.1    61   337\n10  0.23 Very Good H     VS1      59.4    61   338\n# … with 53,930 more rows\n\n\n\n\n\n\nCode\n(df\n.drop(['x', 'y', 'z'], axis = 1)\n)\n\n\n\n\n\n\n\nfiltering on one condition\n\nRPython\n\n\n\n\nCode\nfilter(df, color == 'E')\n\n\n# A tibble: 9,797 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 5  0.2  Premium   E     SI2      60.2    62   345  3.79  3.75  2.27\n 6  0.32 Premium   E     I1       60.9    58   345  4.38  4.42  2.68\n 7  0.23 Very Good E     VS2      63.8    55   352  3.85  3.92  2.48\n 8  0.23 Very Good E     VS1      60.7    59   402  3.97  4.01  2.42\n 9  0.23 Very Good E     VS1      59.5    58   402  4.01  4.06  2.4 \n10  0.23 Good      E     VS1      64.1    59   402  3.83  3.85  2.46\n# … with 9,787 more rows\n\n\n\n\n\n\nCode\n(df\n.query(\"color == 'E'\")\n)\n\n\n\n\n\n\n\nIf we want multiple conditions\n\nRPython\n\n\n\n\nCode\nfilter(df, color == 'E', cut == 'Good')\n\n\n# A tibble: 933 × 10\n   carat cut   color clarity depth table price     x     y     z\n   <dbl> <ord> <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31\n 2  0.23 Good  E     VS1      64.1    59   402  3.83  3.85  2.46\n 3  0.26 Good  E     VVS1     57.9    60   554  4.22  4.25  2.45\n 4  0.7  Good  E     VS2      57.5    58  2759  5.85  5.9   3.38\n 5  0.71 Good  E     VS2      59.2    61  2772  5.8   5.88  3.46\n 6  0.7  Good  E     VS2      64.1    59  2777  5.64  5.59  3.6 \n 7  0.7  Good  E     VS1      57.2    62  2782  5.81  5.77  3.31\n 8  0.76 Good  E     SI1      63.7    54  2789  5.76  5.85  3.7 \n 9  0.7  Good  E     VS2      64.1    55  2793  5.6   5.66  3.61\n10  0.73 Good  E     SI1      63.2    58  2796  5.7   5.76  3.62\n# … with 923 more rows\n\n\nCode\n#or\n#filter(df, color == 'E' & cut == 'Good')\n\n\n\n\n\n\nCode\n(df\n.query('color == \"E\" & cut == \"Good\"')\n)\n\n\n\n\n\n\n\nIf we want multiple conditions in one column\n\nRPython\n\n\n\n\nCode\ndf %>% \n    filter(color %in% c('E', 'J'))\n\n\n# A tibble: 12,605 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 5  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 6  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 7  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n 8  0.23 Ideal     J     VS1      62.8    56   340  3.93  3.9   2.46\n 9  0.31 Ideal     J     SI2      62.2    54   344  4.35  4.37  2.71\n10  0.2  Premium   E     SI2      60.2    62   345  3.79  3.75  2.27\n# … with 12,595 more rows\n\n\n\n\n\n\nCode\n(df\n.query('color in [\"E\", \"J\"]')\n)\n\n\n\n\n\n\n\nCount Missing Values\n\nRPython\n\n\n\n\nCode\n# sum of missing values in each column\ndf %>% \n  summarise(across(everything(), ~sum(is.na(.))))\n\n\n# A tibble: 1 × 10\n  carat   cut color clarity depth table price     x     y     z\n  <int> <int> <int>   <int> <int> <int> <int> <int> <int> <int>\n1     0     0     0       0     0     0     0     0     0     0\n\n\nCode\n#purrr::map_df(df, ~sum(is.na(.)))\n\n\n\n\n\n\nCode\ndf.isna().sum()\n\n\n\n\n\n\n\nCount Unique Values in Each Column\n\nRPython\n\n\n\n\nCode\n# getting the count of unique values in each column \ndf %>% \n  summarise(across(everything(), n_distinct))\n\n#can also map across for the same result\npurrr::map_df(df, ~sum(n_distinct(.)))\n\n# if you just want numerical columns\ndf %>% \n  summarise(across(where(is.numeric), n_distinct))\n\n\n\n\n# A tibble: 1 × 10\n  carat   cut color clarity depth table price     x     y     z\n  <int> <int> <int>   <int> <int> <int> <int> <int> <int> <int>\n1   273     5     7       8   184   127 11602   554   552   375\n\n\n\n\n\n\nCode\ndf.nunique()\n\n# If you want unique values in numeric columns\ndf.select_dtypes(include = np.number).nunique()\n#or\ndf.select_dtypes('number').nunique()\n\n# If you just want the column names of numeric type\ndf.select_dtypes('number').columns\n# If you want them as a list\ndf.select_dtypes(include = np.number).columns.tolist()\n\n# count and unique values\ndf.agg(['count', 'size', 'nunique'])\n\n#for the proportions\ndf.select_dtypes(include = np.number).value_counts(normalize = True)\n#or\ndf.select_dtypes('number').value_counts(normalize = True)\n\n\n\n\n\n\n\n\n\n\nComplex Pipings\n\nRPython\n\n\n\n\nCode\ndf %>%\n  select(starts_with('c')) %>%\n  filter(cut %in% c('Ideal', 'Premium')) %>%\n  group_by(cut, color, clarity) %>%\n  summarise(\n    avgcarat = mean(carat, na.rm=TRUE),\n    n = n()\n    ) %>%\n  arrange(-avgcarat) %>% #desc(avgcarat) also works\n  head()\n\n\n`summarise()` has grouped output by 'cut', 'color'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 5\n# Groups:   cut, color [4]\n  cut     color clarity avgcarat     n\n  <ord>   <ord> <ord>      <dbl> <int>\n1 Ideal   J     I1          1.99     2\n2 Premium I     I1          1.61    24\n3 Premium J     I1          1.58    13\n4 Premium J     SI2         1.55   161\n5 Ideal   H     I1          1.48    38\n6 Premium I     SI2         1.42   312\n\n\n\n\n\n\nCode\n(df\n .filter(regex = '^c')\n .query('cut in [\"Ideal\", \"Premium\"]')\n .groupby(['cut', 'color', 'clarity'])\n .agg(['mean', 'size'])\n .sort_values(by = ('carat', 'mean'), ascending = False)\n .head())\n\n\n\n\n\n\n\nMore Examples\n\nTransforming\n\n\n\n\n\n\n\nR\npandas\n\n\n\n\nselect(df, col_one = col1)\ndf.rename(columns = {'col1': 'col_one'})['col_one']\n\n\nrename(df, col_one = col1)[^2]\ndf.rename(columns = {'col1': 'col_one'})\n\n\nmutate(df, c = a - b)\ndf.assign(c = df['a'] - df['b'])\n\n\n\n\n\nSorting\n\n\n\n\n\n\n\nR\npandas\n\n\n\n\narrange(df, col1, col2)\ndf.sort_values(['col1', 'col2'])\n\n\narrange(df, desc(col1))[^3]\ndf.sort_values('col1', ascending = False)\n\n\n\n\n\nGrouping and Summarising\n\n\n\n\n\n\n\n\nR\npandas\n\n\n\n\nsummary(df)\ndf.describe()\n\n\ngroup_by(df, col1)\ndf.groupby('col1')\n\n\ngroup_by(df, col1) %>% summarise(avg = mean(col1, na.rm = TRUE))\ndf.groupby('col1').agg({'col1' : 'mean'})\n\n\ngroup_by(df, col1) %>% summarise(total = sum(col1))\ndf.groupby('col1').sum()"
  },
  {
    "objectID": "posts/r-test/index.html",
    "href": "posts/r-test/index.html",
    "title": "Dplyr Test",
    "section": "",
    "text": "install.packages(\"renv\")\n#| eval: false\ninstall.packages(\"tidyverse\")\n\nImporting Data, Loading Libraries\n#| warning: false\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf <- diamonds\n\ndf %>% head()\n\n\nSelecting Columns\nselect(df, color, cut)\n\n\nPython\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset('diamonds')\n\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      0.21\n      Premium\n      E\n      SI1\n      59.8\n      61.0\n      326\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      0.23\n      Good\n      E\n      VS1\n      56.9\n      65.0\n      327\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      0.29\n      Premium\n      I\n      VS2\n      62.4\n      58.0\n      334\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      0.31\n      Good\n      J\n      SI2\n      63.3\n      58.0\n      335\n      4.34\n      4.35\n      2.75"
  },
  {
    "objectID": "posts/tidy-pandas/Untitled.html",
    "href": "posts/tidy-pandas/Untitled.html",
    "title": "becausejustyn",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\n\ndf <- diamonds\n\ndf %>% head()\n\n\nAttaching package: ‘dplyr’\n\n\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\nA tibble: 6 × 10\n\n    caratcutcolorclaritydepthtablepricexyz\n    <dbl><ord><ord><ord><dbl><dbl><int><dbl><dbl><dbl>\n\n\n    0.23Ideal    ESI2 61.5553263.953.982.43\n    0.21Premium  ESI1 59.8613263.893.842.31\n    0.23Good     EVS1 56.9653274.054.072.31\n    0.29Premium  IVS2 62.4583344.204.232.63\n    0.31Good     JSI2 63.3583354.344.352.75\n    0.24Very GoodJVVS262.8573363.943.962.48\n\n\n\n\n\ndf.filter(['color', 'cut'])\n\n\n\n\n\n  \n    \n      \n      color\n      cut\n    \n  \n  \n    \n      0\n      E\n      Ideal\n    \n    \n      1\n      E\n      Premium\n    \n    \n      2\n      E\n      Good\n    \n    \n      3\n      I\n      Premium\n    \n    \n      4\n      J\n      Good\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      53935\n      D\n      Ideal\n    \n    \n      53936\n      D\n      Good\n    \n    \n      53937\n      D\n      Very Good\n    \n    \n      53938\n      H\n      Premium\n    \n    \n      53939\n      D\n      Ideal\n    \n  \n\n53940 rows × 2 columns\n\n\n\n\nselect(df, color, cut)\n\n\n\nA tibble: 53940 × 2\n\n    colorcut\n    <ord><ord>\n\n\n    EIdeal    \n    EPremium  \n    EGood     \n    IPremium  \n    JGood     \n    JVery Good\n    IVery Good\n    HVery Good\n    EFair     \n    HVery Good\n    JGood     \n    JIdeal    \n    FPremium  \n    JIdeal    \n    EPremium  \n    EPremium  \n    IIdeal    \n    JGood     \n    JGood     \n    JVery Good\n    IGood     \n    EVery Good\n    HVery Good\n    JVery Good\n    JVery Good\n    GVery Good\n    IPremium  \n    JVery Good\n    DVery Good\n    FVery Good\n    ⋮⋮\n    EPremium  \n    EPremium  \n    FPremium  \n    GGood     \n    IGood     \n    EIdeal    \n    DGood     \n    JVery Good\n    IPremium  \n    IIdeal    \n    EVery Good\n    EVery Good\n    DVery Good\n    IIdeal    \n    IIdeal    \n    IIdeal    \n    EIdeal    \n    FGood     \n    EPremium  \n    GIdeal    \n    EPremium  \n    FPremium  \n    EVery Good\n    EVery Good\n    DPremium  \n    DIdeal    \n    DGood     \n    DVery Good\n    HPremium  \n    DIdeal    \n\n\n\n\n\ndf.loc[:, 'x':'z']\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      4.34\n      4.35\n      2.75\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      53935\n      5.75\n      5.76\n      3.50\n    \n    \n      53936\n      5.69\n      5.75\n      3.61\n    \n    \n      53937\n      5.66\n      5.68\n      3.56\n    \n    \n      53938\n      6.15\n      6.12\n      3.74\n    \n    \n      53939\n      5.83\n      5.87\n      3.64\n    \n  \n\n53940 rows × 3 columns\n\n\n\n\nselect(df, x:z)\n\n\n\nA tibble: 53940 × 3\n\n    xyz\n    <dbl><dbl><dbl>\n\n\n    3.953.982.43\n    3.893.842.31\n    4.054.072.31\n    4.204.232.63\n    4.344.352.75\n    3.943.962.48\n    3.953.982.47\n    4.074.112.53\n    3.873.782.49\n    4.004.052.39\n    4.254.282.73\n    3.933.902.46\n    3.883.842.33\n    4.354.372.71\n    3.793.752.27\n    4.384.422.68\n    4.314.342.68\n    4.234.292.70\n    4.234.262.71\n    4.214.272.66\n    4.264.302.71\n    3.853.922.48\n    3.943.962.41\n    4.394.432.62\n    4.444.472.59\n    3.974.012.41\n    3.973.942.47\n    4.284.302.67\n    3.963.972.40\n    3.963.992.42\n    ⋮⋮⋮\n    5.745.773.48\n    5.435.383.23\n    5.485.403.36\n    5.845.813.74\n    5.945.903.77\n    5.845.863.63\n    5.715.743.61\n    6.126.093.86\n    5.935.853.49\n    5.895.873.66\n    5.575.613.49\n    5.595.653.53\n    5.675.583.55\n    5.805.843.57\n    5.825.843.59\n    5.955.973.67\n    5.715.733.54\n    6.066.133.54\n    6.035.963.68\n    5.765.733.53\n    5.795.743.49\n    5.745.733.43\n    5.715.763.47\n    5.695.723.49\n    5.695.733.58\n    5.755.763.50\n    5.695.753.61\n    5.665.683.56\n    6.156.123.74\n    5.835.873.64\n\n\n\n\n\n(df\n.filter(['color', 'cut'])\n)\n\n\n\n\n\n  \n    \n      \n      color\n      cut\n    \n  \n  \n    \n      0\n      E\n      Ideal\n    \n    \n      1\n      E\n      Premium\n    \n    \n      2\n      E\n      Good\n    \n    \n      3\n      I\n      Premium\n    \n    \n      4\n      J\n      Good\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      53935\n      D\n      Ideal\n    \n    \n      53936\n      D\n      Good\n    \n    \n      53937\n      D\n      Very Good\n    \n    \n      53938\n      H\n      Premium\n    \n    \n      53939\n      D\n      Ideal\n    \n  \n\n53940 rows × 2 columns"
  },
  {
    "objectID": "posts/barplots/index.html",
    "href": "posts/barplots/index.html",
    "title": "Barplots",
    "section": "",
    "text": "I have also been doing a lot of NFL data viz, which I found a lot harder than I thought it would be. I’ll share them at a later date, but I thought I would do a quick post at 11pm at some tricks I learnt with using text labels. I thought I would use the penguins dataset from palmerpenguins since it does not appear to be as common as others such as mtcars.\n\n\nCode\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gt)\n#personal package with plotting theme\nlibrary(becausejustynfun) \n\npenguins <- palmerpenguins::penguins\n\n\nEven though I am using the cleaned version, I find it good practice to use glimpse() just to be safe.\n\n\nCode\npenguins %>% glimpse()\n\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nNext, I am checking for missing values.\n\n\nCode\n#penguins %>%\n#  summarise(across(everything(), ~sum(is.na(.))))\n\npenguins %>% map_dbl(~sum(is.na(.)))\n\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0 \n\n\nTo see how I will deal with them I like to see the unique values across each column.\n\n\nCode\n#penguins %>%\n#  summarise(across(everything(), n_distinct))\n\npenguins %>% \n  map_dbl(~n_distinct(.))\n\n\n          species            island    bill_length_mm     bill_depth_mm \n                3                 3               165                81 \nflipper_length_mm       body_mass_g               sex              year \n               56                95                 3                 3 \n\n\nI’ll have a glance at the missing values to see if they are important.\n\n\nCode\npenguins %>% \n  filter(if_any(everything(), is.na)) \n\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           NA            NA                  NA          NA\n 2 Adelie  Torgersen           34.1          18.1               193        3475\n 3 Adelie  Torgersen           42            20.2               190        4250\n 4 Adelie  Torgersen           37.8          17.1               186        3300\n 5 Adelie  Torgersen           37.8          17.3               180        3700\n 6 Adelie  Dream               37.5          18.9               179        2975\n 7 Gentoo  Biscoe              44.5          14.3               216        4100\n 8 Gentoo  Biscoe              46.2          14.4               214        4650\n 9 Gentoo  Biscoe              47.3          13.8               216        4725\n10 Gentoo  Biscoe              44.5          15.7               217        4875\n11 Gentoo  Biscoe              NA            NA                  NA          NA\n# … with 2 more variables: sex <fct>, year <int>\n\n\nGiven the NA values were low, I will just drop them.\n\n\nCode\npenguins <- penguins %>%\n  na.exclude()\n\n#to check\nmap_dfc(penguins, sum(is.na(penguins)))\n\n\n# A tibble: 0 × 0\n\n\nCode\n#map_df(penguins, .f = sum(is.na(penguins)))\n\n\nI am going to create a seperate df with the count values. You can easily just use add_count(), or just rely on ggplot2 to do that for you, but later on we are going to make more adjustments to the df that will make it easier if there is a mistake.\n\n\nCode\n#df of counts\npenguins_sum <- penguins %>%\n  count(species, sort = TRUE) %>% \n  mutate(\n    species = fct_rev(fct_inorder(species)),\n    # percentage label\n    perc = paste0(sprintf(\"%4.1f\", n / sum(n) * 100), \"%\") \n    )\n\n#scales alternative\n#mutate(\n#    perc = scales::percent(n / sum(n), accuracy = .1, trim = FALSE))\n\n\n\n\nCode\npenguins_sum %>%\nggplot(aes(x = n, y = species)) +\n  geom_col(fill = \"gray70\") +\n  ## add percentage labels\n  geom_text(aes(label = perc)) +\n  white_theme()\n\n\n\n\n\n\n\nCode\n## prepare non-aggregated data set with \n## lumped and ordered factors\npenguins_fct <- penguins %>% \n  dplyr::mutate(\n    total = dplyr::n(),\n    species = stringr::str_to_title(species),\n    species = forcats::fct_rev(forcats::fct_infreq(species))\n  )\n\npenguins_sum <- penguins_sum %>% \n  mutate(\n    colour = case_when(\n      row_number() == 1 ~ \"#468499\",\n      row_number() == 2 ~ \"#E697AC\",\n      row_number() == 3 ~ \"#81CDE6\",\n      ## all others should be gray\n      TRUE ~ \"gray70\"\n    )\n  )\n\n\nThis is one approach were we conditionally colour each variable of choice.\n\n\nCode\npenguins_sum %>%\n  ggplot(aes(\n    x = n, \n    y = species, \n    fill = colour)) +\n  geom_col() +\n  geom_text(\n    aes(label = perc),\n    hjust = 1, nudge_x = -.5\n  ) +\n  ## add custom colors\n  scale_fill_identity(guide = \"none\") +\n  white_theme()\n\n\n\n\n\nWe also have a lot of control over the font used. There can be challenges when installing a font onto your system for the first time, but I might go through that another time since I struggled with it for quite a while. Maybe I am just a silly billy.\n\n\nCode\npenguins_sum %>%\nggplot(aes(\n  x = n, \n  y = species, \n  fill = colour)) +\n  geom_col() +\n  geom_text(\n    aes(label = perc), \n    hjust = 1, nudge_x = -.5,\n    size = 4, fontface = \"bold\", family = \"Fira Sans\"\n  ) +\n  ## reduce spacing between labels and bars\n  scale_x_continuous(expand = c(.01, .01)) +\n  scale_fill_identity(guide = \"none\") +\n  white_theme() \n\n\n\n\n\nSometimes the colour of the font does not match well with the plot. This can be challenging when you have more than a few colours, so you might not want to manually adjust every single one. One option is to add white to the label with fill = \"white\".\n\n\n\nCode\npenguins_sum %>%\n  ggplot(aes(\n    x = n, \n    y = species, \n    fill = colour)) +\n  geom_col() +\n  geom_label(\n    aes(label = perc), \n    hjust = 0.95, nudge_x = -.5,\n    size = 4, fontface = \"bold\",\n    ## turn into white box without outline\n    fill = \"white\", label.size = 0\n  ) +\n  scale_x_continuous(expand = c(.01, .01)) +\n  scale_fill_identity(guide = \"none\") +\n  white_theme() +\n  theme(\n    axis.text.y = element_text(size = 14, hjust = 1, family = \"Fira Sans\"),\n    plot.margin = margin(rep(15, 4))\n  )\n\n\n\n\n\n\nLikewise, it is possible to adjust the position of the text labels conditionally. I think ggplot2 biggest challenge is learning what you can do.\n\n\nCode\npenguins_sum %>% \n  mutate(\n    ## set justification based on data \n    ## so that only the first label is placed inside\n    place = if_else(row_number() == 1, 1, 0),\n    ## add some spacing to labels since we \n    ## cant use nudge_x anymore\n    perc = paste(\" \", perc, \" \")\n  ) %>% \n  ggplot(aes(\n    x = n, \n    y = species, \n    fill = colour)) +\n  geom_col() +\n  geom_text(\n    aes(label = perc, hjust = place), \n    fontface = \"bold\"\n  ) +\n  scale_x_continuous(expand = c(.01, .01)) +\n  scale_fill_identity(guide = \"none\") +\n  white_theme() +\n  theme(\n    plot.margin = margin(rep(15, 4))\n  )\n\n\n\n\n\nYou can use different positions of text labels to highlight things of interest, or sometimes adjusting it makes it easier to work around other features of the plot such as the background or legend.\nThat is all for today. I’ll try to post next week about more data viz.\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] becausejustynfun_0.0.0.9000 gt_0.3.1                   \n [3] janitor_2.1.0               forcats_0.5.1              \n [5] stringr_1.4.0               dplyr_1.0.9                \n [7] purrr_0.3.4                 readr_2.1.2                \n [9] tidyr_1.2.0                 tibble_3.1.7               \n[11] ggplot2_3.3.6               tidyverse_1.3.1            \n[13] palmerpenguins_0.1.0       \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3           lubridate_1.8.0        assertthat_0.2.1      \n [4] digest_0.6.29          utf8_1.2.2             R6_2.5.1              \n [7] cellranger_1.1.0       backports_1.4.1        reprex_2.0.1          \n[10] evaluate_0.15          httr_1.4.3             pillar_1.7.0          \n[13] rlang_1.0.2            readxl_1.3.1           rstudioapi_0.13.0-9000\n[16] rmarkdown_2.14         labeling_0.4.2         htmlwidgets_1.5.4     \n[19] munsell_0.5.0          broom_0.8.0            compiler_4.1.2        \n[22] modelr_0.1.8           xfun_0.31.2            pkgconfig_2.0.3       \n[25] htmltools_0.5.2        tidyselect_1.1.2       fansi_1.0.3           \n[28] crayon_1.5.1           tzdb_0.2.0             dbplyr_2.1.1          \n[31] withr_2.5.0            grid_4.1.2             jsonlite_1.8.0        \n[34] gtable_0.3.0           lifecycle_1.0.1        DBI_1.1.2             \n[37] magrittr_2.0.3         scales_1.2.0           cli_3.3.0             \n[40] stringi_1.7.6          farver_2.1.0           renv_0.15.5           \n[43] fs_1.5.2               ggthemes_4.2.4         snakecase_0.11.0      \n[46] xml2_1.3.3             ellipsis_0.3.2         generics_0.1.2        \n[49] vctrs_0.4.1            tools_4.1.2            glue_1.6.2            \n[52] hms_1.1.1              fastmap_1.1.0          yaml_2.3.5            \n[55] colorspace_2.0-3       rvest_1.0.2            knitr_1.39            \n[58] haven_2.4.3"
  },
  {
    "objectID": "posts/pca-nfl-wr/index.html",
    "href": "posts/pca-nfl-wr/index.html",
    "title": "NFL WR PCA with Tidyverse",
    "section": "",
    "text": "I wont write much on this post due to time constraints, however, most of the code is commented well enough.\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(gt)\nlibrary(ggrepel)\nlibrary(becausejustynfun)\n\n\nHow you load the data depends if you want to use the package nflfastR or not. While it has many useful features, you may not want to use it if you already have the data saved that you want to use.\n\nUsing nflfastRWithout nflfastR\n\n\n\n\nCode\nplayers <- nflfastR::load_player_stats(2021)\nrosters <- nflfastR::fast_scraper_roster(2021)\npbp <- nflfastR::load_pbp(2021)\n\n\nIt is worth noting that you can get multiple seasons at once using this method. E.g. nflfastR::load_player_stats(2020:2021).\n\n\n\n\nCode\n# if you have the data saved locally\ndf <- purrr::map_df(c(2010:2020), function(x) {\n  readRDS(\n    glue::glue(\"~/Documents/nfl/data/pbp/play_by_play_{x}.rds\")\n  )\n})\n\n# if you do not have it saved locally\ndf <- purrr::map_df(c(2010:2020), function(x) {\n  readRDS(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n  )\n})\n\n\nI thought it was worth showing because this was a trick that I found quite useful once I discovered it. I have used it countless times for similar tasks. By using glue(), you can map across the values like an f string in Python.\n\n\n\n\n\nCode\npbp_wr <- pbp |>\n  mutate(\n    # if they caught the ball in the middle of the field, \n    # assign 0 otherwise, assign 1 \n    outside_pass = ifelse(pass_location != \"middle\", 1, 0),\n    pass_air_yards = ifelse(is.na(air_yards), 0, air_yards),\n    pass_air_yards = ifelse(ydstogo <= 10, pass_air_yards, NA)\n  ) |>\n  # rec id is a bit of a hack to keep that value\n  group_by(receiver_id, fantasy_id) |>\n  summarise(\n    rec = sum(complete_pass),\n    air_yards = mean(pass_air_yards, na.rm = TRUE),\n    yards_per_target = mean(yards_gained, na.rm = TRUE),\n    yards_after_catch = mean(yards_after_catch, na.rm = TRUE),\n    td_rate = mean(pass_touchdown),\n    outside_rec = mean(outside_pass, na.rm = TRUE),\n    dist_from_sticks = mean(pass_air_yards - ydstogo, na.rm = TRUE),\n    # first down percentage\n    first_down = mean(first_down, na.rm = TRUE)\n  ) |>\n  # so you don't get random players like a QB\n  filter(rec > 25) |>\n  left_join(\n    pbp |>\n      count(receiver_id, fantasy_id, receiver, posteam) |>\n      group_by(receiver_id) |>\n      arrange(-n) |>\n      # this will keep the first instance of a player\n      # this is to add players non-numerical values\n      mutate(rn = row_number()) |>\n      filter(rn == 1) |>\n      select(-n, -rn)) |>\n  relocate(receiver, .before = rec) |> \n  # this second join is to add the players position\n  # here we are using the gsid, which is why\n  # we wanted the fantasy id before\n  left_join(\n    select(rosters, position, gsis_id), by = c('fantasy_id' = 'gsis_id')\n  ) |>\n  filter(position %in% c('WR', 'TE')) |>\n  # good practice to ungroup at the end\n  ungroup()\n\n\nIf the dataset is large enough, you may get timeout errors, if that is the case, you can seperate the second left_join() into a seperate call, e.g.\n\n\nShow the code\npbp_wr <- pbp |>\n  mutate(\n    outside_pass = ifelse(pass_location != \"middle\", 1, 0),\n    pass_air_yards = ifelse(is.na(air_yards), 0, air_yards),\n    pass_air_yards = ifelse(ydstogo <= 10, pass_air_yards, NA)\n  ) |>\n  group_by(receiver_id, fantasy_id) |>\n  summarise(\n    rec = sum(complete_pass),\n    air_yards = mean(pass_air_yards, na.rm = TRUE),\n    yards_per_target = mean(yards_gained, na.rm = TRUE),\n    yards_after_catch = mean(yards_after_catch, na.rm = TRUE),\n    td_rate = mean(pass_touchdown),\n    outside_rec = mean(outside_pass, na.rm = TRUE),\n    dist_from_sticks = mean(pass_air_yards - ydstogo, na.rm = TRUE),\n    first_down = mean(first_down, na.rm = TRUE)\n  ) |>\n  filter(rec > 25) |>\n  left_join(\n    pbp |>\n      count(receiver_id, fantasy_id, receiver, posteam) |>\n      group_by(receiver_id) |>\n      arrange(-n) |>\n      mutate(rn = row_number()) |>\n      filter(rn == 1) |>\n      select(-n, -rn)) |>\n  relocate(receiver, .before = rec) \n\npbp_wr <- pbp_wr |> \n  left_join(\n    select(rosters, position, gsis_id), by = c('fantasy_id' = 'gsis_id')\n  ) |>\n  filter(position %in% c('WR', 'TE')) |>\n  ungroup()\n\n\n\n\n\nThis is the resulting dataframe. To run a PCA we only want the numeric columns, however, we will be using the character columns for visualising at a later step.\n\n\nCode\nwr_stats |>\n  gt() |>\n  tab_options(container.height = '300px')\n\n\n\n\n\n\n  \n  \n    \n      receiver_id\n      fantasy_id\n      receiver\n      rec\n      air_yards\n      yards_per_target\n      yards_after_catch\n      td_rate\n      outside_rec\n      dist_from_sticks\n      first_down\n      posteam\n      position\n    \n  \n  \n    00-0027061\n00-0027061\nJ.Cook\n48\n7.181818\n6.311111\n4.958333\n0.044444444\n0.8313253\n-0.25974026\n0.3777778\nLAC\nTE\n    00-0027656\n00-0027656\nR.Gronkowski\n64\n9.747475\n8.422018\n6.546875\n0.064220183\n0.6132075\n2.08080808\n0.3853211\nTB\nTE\n    00-0027793\n00-0027793\nA.Brown\n42\n10.500000\n8.384615\n4.523810\n0.061538462\n0.9193548\n3.12500000\n0.4769231\nTB\nWR\n    00-0027944\n00-0027944\nJ.Jones\n37\n11.296296\n8.701754\n3.675676\n0.017543860\n0.7272727\n4.01851852\n0.4035088\nTEN\nWR\n    00-0027981\n00-0027981\nK.Rudolph\n26\n5.714286\n6.166667\n5.269231\n0.023809524\n0.7179487\n-1.83333333\n0.3809524\nNYG\nTE\n    00-0028002\n00-0028002\nR.Cobb\n28\n7.513514\n9.375000\n4.928571\n0.125000000\n0.6500000\n0.81081081\n0.5250000\nGB\nWR\n    00-0029000\n00-0029000\nC.Beasley\n89\n4.651376\n6.094488\n3.853933\n0.007874016\n0.7438017\n-3.37614679\n0.2992126\nBUF\nWR\n    00-0030035\n00-0030035\nA.Thielen\n67\n9.216867\n7.117647\n3.567164\n0.098039216\n0.8736842\n1.54216867\n0.3823529\nMIN\nWR\n    00-0030061\n00-0030061\nZ.Ertz\n77\n6.423077\n6.426230\n4.571429\n0.040983607\n0.6379310\n-0.95192308\n0.3606557\nARI\nTE\n    00-0030108\n00-0030108\nR.Griffin\n27\n5.000000\n5.800000\n5.518519\n0.044444444\n0.7619048\n-3.05000000\n0.3333333\nNYJ\nTE\n    00-0030181\n00-0030181\nJ.Doyle\n29\n5.925000\n6.608696\n4.310345\n0.065217391\n0.6976744\n-1.22500000\n0.4347826\nIND\nTE\n    00-0030279\n00-0030279\nK.Allen\n106\n7.326797\n6.733728\n3.179245\n0.035502959\n0.7579618\n-0.03921569\n0.4082840\nLAC\nWR\n    00-0030431\n00-0030431\nR.Woods\n45\n7.538462\n7.750000\n4.400000\n0.055555556\n0.6811594\n-0.23076923\n0.5000000\nLA\nWR\n    00-0030506\n00-0030506\nT.Kelce\n105\n6.706294\n8.109091\n6.304762\n0.066666667\n0.6375839\n-1.12587413\n0.4969697\nKC\nTE\n    00-0030564\n00-0030564\nD.Hopkins\n42\n10.967213\n8.056338\n3.333333\n0.112676056\n0.8750000\n3.73770492\n0.5070423\nARI\nWR\n    00-0031235\n00-0031235\nO.Beckham\n54\n11.655172\n6.534653\n3.592593\n0.059405941\n0.7872340\n4.52873563\n0.3861386\nLA\nWR\n    00-0031236\n00-0031236\nB.Cooks\n90\n9.791667\n7.137931\n3.944444\n0.041379310\n0.8507463\n2.45833333\n0.3655172\nHOU\nWR\n    00-0031273\n00-0031273\nC.Brate\n34\n6.948276\n4.421875\n3.147059\n0.062500000\n0.5714286\n-0.48275862\n0.2812500\nTB\nTE\n    00-0031325\n00-0031325\nS.Watkins\n27\n12.565217\n8.040816\n3.666667\n0.020408163\n0.6326531\n3.95652174\n0.3469388\nBAL\nWR\n    00-0031381\n00-0031381\nD.Adams\n132\n8.576271\n8.557292\n4.803030\n0.057291667\n0.8111111\n0.88700565\n0.5052083\nGB\nWR\n    00-0031382\n00-0031382\nJ.Landry\n52\n7.298701\n6.406593\n5.500000\n0.021978022\n0.8505747\n-0.87012987\n0.3626374\nCLE\nWR\n    00-0031408\n00-0031408\nM.Evans\n91\n11.540741\n8.705479\n3.494505\n0.109589041\n0.7785714\n3.98518519\n0.4863014\nTB\nWR\n    00-0031428\n00-0031428\nA.Robinson\n38\n9.863636\n5.567568\n2.657895\n0.013513514\n0.8181818\n2.25757576\n0.3918919\nCHI\nWR\n    00-0031544\n00-0031544\nA.Cooper\n74\n9.455357\n7.552846\n3.216216\n0.073170732\n0.8508772\n1.75000000\n0.4308943\nDAL\nWR\n    00-0031547\n00-0031547\nD.Parker\n40\n10.300000\n6.602564\n2.600000\n0.025641026\n0.7808219\n2.47142857\n0.4230769\nMIA\nWR\n    00-0031549\n00-0031549\nN.Agholor\n38\n13.158730\n6.726027\n3.210526\n0.041095890\n0.8030303\n4.58730159\n0.4109589\nNE\nWR\n    00-0031610\n00-0031610\nD.Waller\n62\n9.000000\n6.798165\n4.161290\n0.018348624\n0.7428571\n0.97916667\n0.3394495\nLV\nTE\n    00-0031868\n00-0031868\nD.Byrd\n26\n7.833333\n8.275000\n4.461538\n0.025000000\n0.8157895\n0.22222222\n0.4750000\nCHI\nWR\n    00-0031941\n00-0031941\nJ.Crowder\n51\n5.919355\n6.608108\n3.411765\n0.027027027\n0.8169014\n-1.38709677\n0.3243243\nNYJ\nWR\n    00-0032009\n00-0032009\nA.Humphries\n41\n5.118644\n5.471429\n3.658537\n0.000000000\n0.8064516\n-2.72881356\n0.4142857\nWAS\nWR\n    00-0032134\n00-0032134\nC.Uzomah\n62\n4.620690\n8.051282\n5.903226\n0.076923077\n0.7402597\n-3.12068966\n0.3974359\nCIN\nTE\n    00-0032141\n00-0032141\nG.Swaim\n31\n3.540541\n4.883721\n4.483871\n0.069767442\n0.7250000\n-4.05405405\n0.3023256\nTEN\nTE\n    00-0032211\n00-0032211\nT.Lockett\n73\n13.310680\n10.059829\n3.794521\n0.068376068\n0.8878505\n5.34951456\n0.4102564\nSEA\nWR\n    00-0032355\n00-0032355\nC.Rogers\n31\n6.162162\n5.960784\n3.451613\n0.019607843\n0.6888889\n-1.81081081\n0.3137255\nTEN\nWR\n    00-0032385\n00-0032385\nS.Shepard\n36\n7.240741\n6.203390\n3.083333\n0.016949153\n0.6792453\n-1.03703704\n0.4237288\nNYG\nWR\n    00-0032392\n00-0032392\nA.Hooper\n38\n4.810345\n5.179104\n4.868421\n0.044776119\n0.7704918\n-2.58620690\n0.3283582\nCLE\nTE\n    00-0032464\n00-0032464\nK.Raymond\n48\n10.453125\n8.027397\n5.458333\n0.054794521\n0.7323944\n2.89062500\n0.4246575\nDET\nWR\n    00-0032688\n00-0032688\nR.Anderson\n53\n10.083333\n4.552632\n2.981132\n0.043859649\n0.8454545\n1.96875000\n0.2543860\nCAR\nWR\n    00-0032775\n00-0032775\nD.Robinson\n29\n10.368421\n6.938776\n3.103448\n0.061224490\n0.7872340\n1.97368421\n0.3265306\nKC\nWR\n    00-0032951\n00-0032951\nL.Treadwell\n33\n11.340426\n8.037037\n3.818182\n0.018518519\n0.7843137\n3.87234043\n0.4444444\nJAX\nWR\n    00-0033009\n00-0033009\nT.Boyd\n73\n6.936170\n8.140187\n5.424658\n0.056074766\n0.5980392\n-0.28723404\n0.4018692\nCIN\nWR\n    00-0033110\n00-0033110\nT.Higbee\n68\n5.454545\n6.704082\n4.588235\n0.051020408\n0.7812500\n-2.23863636\n0.4081633\nLA\nTE\n    00-0033251\n00-0033251\nZ.Pascal\n38\n8.155172\n5.472222\n3.184211\n0.041666667\n0.6811594\n-0.68965517\n0.2777778\nIND\nWR\n    00-0033288\n00-0033288\nG.Kittle\n76\n8.183908\n9.621359\n6.381579\n0.058252427\n0.5728155\n-0.12643678\n0.4563107\nSF\nTE\n    00-0033307\n00-0033307\nK.Bourne\n62\n8.397059\n10.566265\n6.564516\n0.084337349\n0.7948718\n0.64705882\n0.4337349\nNE\nWR\n    00-0033375\n00-0033375\nT.Patrick\n53\n11.628205\n8.340909\n3.811321\n0.056818182\n0.7647059\n3.51282051\n0.4659091\nDEN\nWR\n    00-0033455\n00-0033455\nA.Firkser\n34\n5.270270\n6.466667\n3.264706\n0.044444444\n0.4545455\n-1.78378378\n0.3555556\nTEN\nTE\n    00-0033536\n00-0033536\nM.Williams\n76\n10.392000\n8.258993\n5.460526\n0.064748201\n0.8604651\n2.84000000\n0.4028777\nLAC\nWR\n    00-0033611\n00-0033611\nR.Seals-Jones\n30\n4.375000\n4.839286\n5.233333\n0.035714286\n0.7755102\n-2.97916667\n0.2678571\nWAS\nTE\n    00-0033681\n00-0033681\nK.Cole\n28\n12.020408\n8.017857\n3.035714\n0.017857143\n0.6862745\n4.00000000\n0.4285714\nNYJ\nWR\n    00-0033858\n00-0033858\nJ.Smith\n28\n6.450000\n6.000000\n8.285714\n0.020408163\n0.8000000\n-2.42500000\n0.3265306\nNE\nTE\n    00-0033871\n00-0033871\nC.Davis\n34\n11.966102\n7.687500\n3.647059\n0.062500000\n0.8305085\n3.93220339\n0.4218750\nNYJ\nWR\n    00-0033881\n00-0033881\nE.Engram\n46\n5.261538\n5.324675\n4.021739\n0.038961039\n0.6986301\n-2.52307692\n0.3116883\nNYG\nTE\n    00-0033885\n00-0033885\nD.Njoku\n36\n6.220000\n8.333333\n6.861111\n0.070175439\n0.7547170\n-1.50000000\n0.3333333\nCLE\nTE\n    00-0033891\n00-0033891\nZ.Jones\n52\n11.507246\n7.402439\n3.307692\n0.024390244\n0.8076923\n3.94202899\n0.4268293\nLV\nWR\n    00-0033895\n00-0033895\nG.Everett\n48\n5.196721\n7.242424\n5.187500\n0.060606061\n0.7460317\n-2.32786885\n0.4242424\nSEA\nTE\n    00-0033908\n00-0033908\nC.Kupp\n159\n7.801020\n9.968182\n5.962264\n0.081818182\n0.7894737\n0.19897959\n0.4727273\nLA\nWR\n    00-0033932\n00-0033932\nK.Golladay\n37\n12.480000\n6.202381\n3.216216\n0.000000000\n0.7105263\n4.29333333\n0.3690476\nNYG\nWR\n    00-0033943\n00-0033943\nJ.Reynolds\n29\n11.391304\n7.615385\n4.103448\n0.038461538\n0.8367347\n3.45652174\n0.4615385\nDET\nWR\n    00-0034011\n00-0034011\nD.Arnold\n35\n5.895833\n7.192982\n5.371429\n0.000000000\n0.7500000\n-1.41666667\n0.4035088\nJAX\nTE\n    00-0034270\n00-0034270\nT.Conklin\n61\n4.757143\n6.516484\n5.344262\n0.032967033\n0.6781609\n-3.00000000\n0.3076923\nMIN\nTE\n    00-0034272\n00-0034272\nM.Valdes-Scantling\n26\n16.229167\n7.543860\n5.615385\n0.052631579\n0.8363636\n9.37500000\n0.3333333\nGB\nWR\n    00-0034297\n00-0034297\nB.Pringle\n52\n9.728571\n8.342105\n4.057692\n0.105263158\n0.7027027\n2.51428571\n0.5000000\nKC\nWR\n    00-0034351\n00-0034351\nD.Goedert\n62\n8.244444\n9.726316\n6.774194\n0.042105263\n0.9318182\n0.67777778\n0.5157895\nPHI\nTE\n    00-0034383\n00-0034383\nD.Schultz\n85\n6.865385\n7.683761\n4.458824\n0.068376068\n0.7500000\n-0.78846154\n0.4188034\nDAL\nTE\n    00-0034407\n00-0034407\nR.McCloud\n41\n6.076923\n4.352113\n3.000000\n0.000000000\n0.6470588\n-1.46153846\n0.2816901\nPIT\nWR\n    00-0034411\n00-0034411\nR.Gage\n66\n8.775000\n8.115789\n3.848485\n0.042105263\n0.6702128\n1.20000000\n0.4000000\nATL\nWR\n    00-0034418\n00-0034418\nC.Wilson\n50\n10.000000\n9.026667\n5.620000\n0.080000000\n0.6901408\n2.73333333\n0.4133333\nDAL\nWR\n    00-0034419\n00-0034419\nB.Berrios\n46\n5.067797\n6.196970\n5.260870\n0.030303030\n0.7384615\n-2.94915254\n0.3181818\nNYJ\nWR\n    00-0034521\n00-0034521\nA.Lazard\n41\n9.446429\n8.109375\n4.097561\n0.125000000\n0.7540984\n2.39285714\n0.5000000\nGB\nWR\n    00-0034676\n00-0034676\nJ.Washington\n26\n12.243902\n6.192308\n3.153846\n0.057692308\n0.8510638\n3.75609756\n0.2884615\nPIT\nWR\n    00-0034753\n00-0034753\nM.Andrews\n107\n9.412587\n8.272727\n4.280374\n0.054545455\n0.5947712\n1.29370629\n0.4727273\nBAL\nTE\n    00-0034764\n00-0034764\nM.Gallup\n35\n10.411765\n6.357143\n3.514286\n0.028571429\n0.8709677\n3.20588235\n0.4000000\nDAL\nWR\n    00-0034765\n00-0034765\nT.Smith\n32\n8.666667\n6.732143\n3.531250\n0.053571429\n0.7800000\n1.22916667\n0.3750000\nNO\nWR\n    00-0034775\n00-0034775\nC.Kirk\n83\n11.312500\n8.965812\n3.085366\n0.042735043\n0.7192982\n3.82291667\n0.4017094\nARI\nWR\n    00-0034798\n00-0034798\nD.Smythe\n34\n6.051282\n8.302326\n4.705882\n0.000000000\n0.7804878\n-1.97435897\n0.3488372\nMIA\nTE\n    00-0034827\n00-0034827\nDj.Moore\n93\n9.813793\n6.738372\n4.630435\n0.023255814\n0.7839506\n1.83448276\n0.3720930\nCAR\nWR\n    00-0034830\n00-0034830\nH.Hurst\n26\n6.392857\n6.696970\n3.423077\n0.090909091\n0.7096774\n-1.32142857\n0.3636364\nATL\nTE\n    00-0034837\n00-0034837\nC.Ridley\n31\n9.360000\n5.017857\n2.935484\n0.035714286\n0.7500000\n1.66000000\n0.3750000\nATL\nWR\n    00-0034960\n00-0034960\nJ.Meyers\n89\n8.876923\n6.363636\n2.539326\n0.013986014\n0.7481481\n1.63076923\n0.3496503\nNE\nWR\n    00-0034981\n00-0034981\nF.Moreau\n30\n7.022727\n7.313725\n6.300000\n0.058823529\n0.7659574\n-0.70454545\n0.3725490\nLV\nTE\n    00-0034983\n00-0034983\nH.Renfrow\n111\n5.822581\n7.506849\n4.387387\n0.061643836\n0.8057554\n-1.61290323\n0.3972603\nLV\nWR\n    00-0035140\n00-0035140\nM.Hardman\n64\n6.341176\n8.106383\n8.828125\n0.021276596\n0.7727273\n-1.21176471\n0.4042553\nKC\nWR\n    00-0035208\n00-0035208\nO.Zaccheaus\n31\n8.125000\n7.250000\n3.096774\n0.053571429\n0.7169811\n1.16666667\n0.4107143\nATL\nWR\n    00-0035215\n00-0035215\nD.Harris\n36\n9.372549\n9.193548\n6.694444\n0.048387097\n0.8305085\n0.78431373\n0.3870968\nNO\nWR\n    00-0035229\n00-0035229\nT.Hockenson\n61\n6.719512\n6.358696\n3.262295\n0.043478261\n0.7619048\n-0.90243902\n0.3913043\nDET\nTE\n    00-0035414\n00-0035414\nJ.Guyton\n31\n8.418605\n8.960000\n4.806452\n0.060000000\n0.8958333\n0.67441860\n0.4600000\nLAC\nWR\n    00-0035535\n00-0035535\nD.Slayton\n26\n13.000000\n5.380952\n2.500000\n0.031746032\n0.8448276\n4.71929825\n0.2857143\nNYG\nWR\n    00-0035640\n00-0035640\nD.Metcalf\n75\n11.256000\n7.110294\n4.373333\n0.088235294\n0.8217054\n3.48000000\n0.3750000\nSEA\nWR\n    00-0035644\n00-0035644\nN.Fant\n68\n6.486842\n7.282609\n4.544118\n0.043478261\n0.8666667\n-1.98684211\n0.3152174\nDEN\nTE\n    00-0035659\n00-0035659\nT.McLaurin\n77\n12.391667\n7.742647\n4.116883\n0.036764706\n0.7769231\n4.44166667\n0.3823529\nWAS\nWR\n    00-0035662\n00-0035662\nM.Brown\n91\n10.511111\n6.461538\n4.263736\n0.038461538\n0.7602740\n2.28888889\n0.2884615\nBAL\nWR\n    00-0035676\n00-0035676\nA.Brown\n68\n11.739583\n8.641026\n3.867647\n0.051282051\n0.6929825\n3.86458333\n0.4529915\nTEN\nWR\n    00-0035689\n00-0035689\nD.Knox\n56\n6.670732\n7.873563\n5.285714\n0.126436782\n0.8000000\n-1.32926829\n0.4137931\nBUF\nTE\n    00-0036182\n00-0036182\nN.Westbrook-Ikhine\n38\n10.140000\n7.803279\n3.789474\n0.065573770\n0.6896552\n3.02000000\n0.4098361\nTEN\nWR\n    00-0036196\n00-0036196\nG.Davis\n45\n13.147059\n10.141026\n3.866667\n0.141025641\n0.7631579\n5.27941176\n0.5128205\nBUF\nWR\n    00-0036219\n00-0036219\nM.Callaway\n46\n11.788732\n7.931818\n3.130435\n0.068181818\n0.8452381\n4.04225352\n0.3863636\nNO\nWR\n    00-0036233\n00-0036233\nD.Peoples-Jones\n34\n15.134615\n9.629032\n4.294118\n0.048387097\n0.8448276\n7.44230769\n0.4677419\nCLE\nWR\n    00-0036252\n00-0036252\nM.Pittman\n88\n8.215517\n7.912409\n3.954545\n0.043795620\n0.8139535\n0.55172414\n0.4233577\nIND\nWR\n    00-0036259\n00-0036259\nJ.Jennings\n28\n9.342105\n6.891304\n4.000000\n0.108695652\n0.6888889\n2.18421053\n0.3913043\nSF\nWR\n    00-0036261\n00-0036261\nB.Aiyuk\n61\n8.630952\n9.020000\n6.196721\n0.050000000\n0.7142857\n1.00000000\n0.4500000\nSF\nWR\n    00-0036268\n00-0036268\nL.Shenault\n63\n4.963415\n5.951923\n6.174603\n0.000000000\n0.8200000\n-2.45121951\n0.3076923\nJAX\nWR\n    00-0036271\n00-0036271\nQ.Watkins\n45\n11.761905\n9.243243\n5.711111\n0.013513514\n0.8840580\n3.96825397\n0.3648649\nPHI\nWR\n    00-0036290\n00-0036290\nC.Kmet\n60\n7.320513\n6.244898\n4.100000\n0.000000000\n0.6021505\n-0.87179487\n0.3061224\nCHI\nTE\n    00-0036309\n00-0036309\nD.Mooney\n81\n10.546875\n7.226027\n4.703704\n0.027397260\n0.7714286\n3.00000000\n0.3630137\nCHI\nWR\n    00-0036322\n00-0036322\nJ.Jefferson\n108\n12.019481\n8.983333\n4.462963\n0.055555556\n0.7844311\n4.54545455\n0.4555556\nMIN\nWR\n    00-0036326\n00-0036326\nC.Claypool\n62\n11.201923\n7.040323\n4.725806\n0.016129032\n0.8035714\n4.30769231\n0.3548387\nPIT\nWR\n    00-0036331\n00-0036331\nD.Duvernay\n33\n5.048780\n5.440000\n4.272727\n0.040000000\n0.6170213\n-2.43902439\n0.3000000\nBAL\nWR\n    00-0036345\n00-0036345\nK.Osborn\n50\n9.542857\n7.277778\n3.580000\n0.077777778\n0.7926829\n1.12857143\n0.3333333\nMIN\nWR\n    00-0036358\n00-0036358\nC.Lamb\n80\n9.276423\n8.159420\n5.512500\n0.043478261\n0.8240000\n1.62601626\n0.4347826\nDAL\nWR\n    00-0036365\n00-0036365\nB.Edwards\n37\n12.238095\n8.500000\n5.567568\n0.041666667\n0.6307692\n4.87301587\n0.4444444\nLV\nWR\n    00-0036387\n00-0036387\nJ.Reagor\n34\n8.840000\n4.703125\n4.588235\n0.031250000\n0.9166667\n0.76000000\n0.2343750\nPHI\nWR\n    00-0036407\n00-0036407\nJ.Jeudy\n38\n8.936170\n7.783333\n4.868421\n0.000000000\n0.6964286\n1.19148936\n0.4166667\nDEN\nWR\n    00-0036410\n00-0036410\nT.Higgins\n82\n10.074766\n9.152672\n4.121951\n0.045801527\n0.7560976\n2.70093458\n0.4961832\nCIN\nWR\n    00-0036415\n00-0036415\nV.Jefferson\n53\n12.436170\n8.643564\n4.528302\n0.059405941\n0.7204301\n5.04255319\n0.3861386\nLA\nWR\n    00-0036422\n00-0036422\nA.Trautman\n27\n5.081081\n5.977273\n4.111111\n0.045454545\n0.7209302\n-2.70270270\n0.2954545\nNO\nTE\n    00-0036423\n00-0036423\nA.Okwuegbunam\n33\n5.526316\n7.857143\n7.424242\n0.047619048\n0.8250000\n-2.86842105\n0.3809524\nDEN\nTE\n    00-0036427\n00-0036427\nT.Johnson\n41\n6.844828\n6.230769\n4.048780\n0.000000000\n0.6875000\n-0.84482759\n0.2615385\nTB\nWR\n    00-0036550\n00-0036550\nR.Bateman\n46\n8.463768\n7.152778\n3.717391\n0.013888889\n0.7058824\n0.34782609\n0.4444444\nBAL\nWR\n    00-0036554\n00-0036554\nN.Collins\n33\n10.166667\n7.000000\n4.030303\n0.015625000\n0.8666667\n2.70000000\n0.3750000\nHOU\nWR\n    00-0036613\n00-0036613\nJ.Waddle\n104\n6.348485\n6.766667\n4.221154\n0.040000000\n0.7714286\n-1.00000000\n0.4466667\nMIA\nWR\n    00-0036894\n00-0036894\nP.Freiermuth\n64\n4.680000\n6.022989\n4.187500\n0.080459770\n0.7023810\n-2.45333333\n0.4022989\nPIT\nTE\n    00-0036900\n00-0036900\nJ.Chase\n95\n11.873134\n10.909091\n8.021053\n0.084415584\n0.8082192\n4.16417910\n0.4675325\nCIN\nWR\n    00-0036912\n00-0036912\nD.Smith\n68\n11.754902\n7.967480\n3.970588\n0.040650407\n0.8869565\n3.86274510\n0.3902439\nPHI\nWR\n    00-0036913\n00-0036913\nK.Toney\n39\n5.698113\n7.118644\n5.846154\n0.000000000\n0.8947368\n-2.15094340\n0.3728814\nNYG\nWR\n    00-0036936\n00-0036936\nR.Moore\n59\n1.603448\n6.577465\n7.694915\n0.014084507\n0.8428571\n-6.65517241\n0.3098592\nARI\nWR\n    00-0036963\n00-0036963\nA.St\n90\n6.203540\n7.269841\n4.711111\n0.039682540\n0.7983193\n-1.03539823\n0.3888889\nDET\nWR\n    00-0036970\n00-0036970\nK.Pitts\n68\n10.196078\n8.844828\n4.602941\n0.008620690\n0.6909091\n2.21568627\n0.4051724\nATL\nTE\n    00-0036980\n00-0036980\nE.Moore\n43\n10.676056\n6.481928\n4.813953\n0.060240964\n0.7532468\n2.71830986\n0.3012048\nNYJ\nWR\n    00-0036988\n00-0036988\nJ.Palmer\n33\n9.204545\n7.204082\n2.848485\n0.081632653\n0.8979592\n1.84090909\n0.3673469\nLAC\nWR\n  \n  \n  \n\n\n\n\n\n\nCode\npca_fit <- wr_stats |>\n  # only keep numerical columns\n  select(where(is.numeric)) |> \n  # scale data, prcomp(scale = TRUE) also works\n  scale() |> \n  # PCA\n  prcomp() \n\npca_fit\n\n\nStandard deviations (1, .., p=8):\n[1] 1.69935082 1.33346010 1.02217972 0.92777639 0.87569306 0.70127121 0.39987632\n[8] 0.09974876\n\nRotation (n x k) = (8 x 8):\n                          PC1        PC2         PC3         PC4         PC5\nrec                0.16137996 -0.3639617 -0.02410570 -0.82296579  0.31561563\nair_yards          0.47287056  0.3992487 -0.05940229 -0.06691055 -0.18768212\nyards_per_target   0.47412999 -0.3318346 -0.04840203  0.04464987 -0.26974551\nyards_after_catch -0.02119252 -0.5509070 -0.41848548  0.24055597 -0.41532072\ntd_rate            0.28807638 -0.2122223  0.35011339  0.48717240  0.59281064\noutside_rec        0.15281538  0.1133213 -0.81216385  0.12452691  0.49041555\ndist_from_sticks   0.48430450  0.3855597 -0.02419456 -0.06080339 -0.16612487\nfirst_down         0.42919714 -0.2966200  0.18879233  0.04299475 -0.01946589\n                          PC6         PC7          PC8\nrec               -0.23954562  0.08137319  0.004208327\nair_yards         -0.23947634  0.14017085  0.704816946\nyards_per_target   0.05965311 -0.76390598 -0.029608784\nyards_after_catch -0.37839015  0.38390354 -0.001658467\ntd_rate           -0.39875893  0.03611646  0.017884466\noutside_rec        0.21306905 -0.05178428 -0.010061644\ndist_from_sticks  -0.21135594  0.20064907 -0.707093783\nfirst_down         0.69960914  0.44558972  0.043968435\n\n\n\n\nCode\n# helper for the axis labels\nimportance <- pca_fit |> \n  tidy(matrix = \"eigenvalues\") |> \n  filter(PC %in% c(1, 2)) |> \n  pull(percent) |> \n  round(3)\n\npca_fit |>\n  # add original dataset back in\n  augment(wr_stats) |> \n  ggplot(aes(\n    .fittedPC1, \n    .fittedPC2, \n    colour = position)) + \n  geom_point(size = 1.5) +\n  labs(\n    x = paste0('PC1 (Accounts for ', importance[[1]]*100, '% of Variance)'), \n    y = paste0('PC2 (Accounts for ', importance[[2]]*100, '% of Variance)') \n    ) +\n  becausejustynfun::white_theme()\n\n\n\n\n\n\n\nCode\npca_fit |>\n  tidy(matrix = \"rotation\") |>\n  pivot_wider(\n    names_from = \"PC\", \n    names_prefix = \"PC\", \n    values_from = \"value\") |>\n  gt() |>\n  tab_options(container.height = '500px')\n\n\n\n\n\n\n  \n  \n    \n      column\n      PC1\n      PC2\n      PC3\n      PC4\n      PC5\n      PC6\n      PC7\n      PC8\n    \n  \n  \n    rec\n0.16137996\n-0.3639617\n-0.02410570\n-0.82296579\n0.31561563\n-0.23954562\n0.08137319\n0.004208327\n    air_yards\n0.47287056\n0.3992487\n-0.05940229\n-0.06691055\n-0.18768212\n-0.23947634\n0.14017085\n0.704816946\n    yards_per_target\n0.47412999\n-0.3318346\n-0.04840203\n0.04464987\n-0.26974551\n0.05965311\n-0.76390598\n-0.029608784\n    yards_after_catch\n-0.02119252\n-0.5509070\n-0.41848548\n0.24055597\n-0.41532072\n-0.37839015\n0.38390354\n-0.001658467\n    td_rate\n0.28807638\n-0.2122223\n0.35011339\n0.48717240\n0.59281064\n-0.39875893\n0.03611646\n0.017884466\n    outside_rec\n0.15281538\n0.1133213\n-0.81216385\n0.12452691\n0.49041555\n0.21306905\n-0.05178428\n-0.010061644\n    dist_from_sticks\n0.48430450\n0.3855597\n-0.02419456\n-0.06080339\n-0.16612487\n-0.21135594\n0.20064907\n-0.707093783\n    first_down\n0.42919714\n-0.2966200\n0.18879233\n0.04299475\n-0.01946589\n0.69960914\n0.44558972\n0.043968435\n  \n  \n  \n\n\n\n\n\n\nCode\n# define arrow style for plotting\narrow_style <- arrow(\n  angle = 20, \n  ends = \"first\", \n  type = \"closed\", \n  length = grid::unit(8, \"pt\")\n  )\n\n\n\n\nCode\n# plot rotation matrix\npca_fit |>\n  tidy(matrix = \"rotation\") |>\n  pivot_wider(\n    names_from = \"PC\", \n    names_prefix = \"PC\", \n    values_from = \"value\") |>\n  ggplot(aes(PC1, PC2)) +\n  scale_color_brewer(palette = \"Accent\", direction = 1) + \n  geom_segment(\n    aes(colour = column),\n    xend = 0, \n    yend = 0, \n    arrow = arrow_style) +\n  geom_text_repel( \n    aes(label = column, colour = column),\n    hjust = 1, \n    nudge_x = -0.02\n  ) +\n  scale_x_continuous(limits = c(-1, 1)) + \n  scale_y_continuous(limits = c(-1, 1)) +\n  becausejustynfun::theme_dark_blue() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nI used a dark theme for this plot because of the different colours used for the components.\n\n\nCode\npca_fit |>\n  tidy(matrix = \"eigenvalues\") |>\n  gt() |>\n  tab_options(container.height = '500px')\n\n\n\n\n\n\n  \n  \n    \n      PC\n      std.dev\n      percent\n      cumulative\n    \n  \n  \n    1\n1.69935082\n0.36097\n0.36097\n    2\n1.33346010\n0.22226\n0.58324\n    3\n1.02217972\n0.13061\n0.71385\n    4\n0.92777639\n0.10760\n0.82144\n    5\n0.87569306\n0.09585\n0.91730\n    6\n0.70127121\n0.06147\n0.97877\n    7\n0.39987632\n0.01999\n0.99876\n    8\n0.09974876\n0.00124\n1.00000\n  \n  \n  \n\n\n\n\n\n\nCode\npca_fit |>\n  tidy(matrix = \"eigenvalues\") |>\n  ggplot(aes(PC, percent)) +\n  geom_col(fill = \"#56B4E9\", alpha = 0.8) +\n  scale_x_continuous(breaks = 1:8) +\n  scale_y_continuous(\n    breaks = seq(0, 0.40, 0.05),\n    labels = scales::percent_format(), \n    expand = expansion(mult = c(0, 0.01))\n  ) +\n  becausejustynfun::white_theme()\n\n\n\n\n\n\nUpdate\nI thought it might be fun to add an example of a 3D plot of the components. For this I will use plotly. It is one of the more interactive friendly visualisation tools, and from my experience the code is very similar from Python to R. If you are not too familiar with Python, writing this code may feel awkward. However, I think it is quite useful to get familiar with it because I can easily use plotly in Python without having to change much at all.\n\n\nCode\nlibrary(plotly)\n\n# making a seperate df will simplify plotly creation\npca_df <- pca_fit |>\n  augment(wr_stats)\n\nfig <- plot_ly(\n  pca_df, \n  x = ~.fittedPC1, \n  y = ~.fittedPC2, \n  z = ~.fittedPC3, \n  color = ~position, \n  colors = c('#386cb0', '#beaed4'), \n  text = ~receiver)\n\nfig <- fig |> \n  add_markers()\n\nfig <- fig |> \n  layout(scene = list(\n    xaxis = list(title = 'PC1'),\n    yaxis = list(title = 'PC2'),\n    zaxis = list(title = 'PC3')\n    ))\n\n\n\n\nCode\nfig\n\n\n\n\n\n\nThat is all for now. I have a few more similar posts saved in my draft, so they should be on here soon (within the next six months).\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] plotly_4.10.0               becausejustynfun_0.0.0.9000\n [3] ggrepel_0.9.1               gt_0.3.1                   \n [5] broom_0.8.0                 forcats_0.5.1              \n [7] stringr_1.4.0               dplyr_1.0.9                \n [9] purrr_0.3.4                 readr_2.1.2                \n[11] tidyr_1.2.0                 tibble_3.1.7               \n[13] ggplot2_3.3.6               tidyverse_1.3.1            \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.3             sass_0.4.1             bit64_4.0.5           \n [4] vroom_1.5.7            jsonlite_1.8.0         viridisLite_0.4.0     \n [7] modelr_0.1.8           assertthat_0.2.1       renv_0.15.5           \n[10] cellranger_1.1.0       yaml_2.3.5             pillar_1.7.0          \n[13] backports_1.4.1        glue_1.6.2             digest_0.6.29         \n[16] RColorBrewer_1.1-3     checkmate_2.0.0        rvest_1.0.2           \n[19] colorspace_2.0-3       htmltools_0.5.2        pkgconfig_2.0.3       \n[22] haven_2.4.3            scales_1.2.0           tzdb_0.2.0            \n[25] generics_0.1.2         farver_2.1.0           ellipsis_0.3.2        \n[28] withr_2.5.0            lazyeval_0.2.2         cli_3.3.0             \n[31] magrittr_2.0.3         crayon_1.5.1           readxl_1.3.1          \n[34] evaluate_0.15          fs_1.5.2               fansi_1.0.3           \n[37] xml2_1.3.3             ggthemes_4.2.4         tools_4.1.2           \n[40] data.table_1.14.2      hms_1.1.1              lifecycle_1.0.1       \n[43] munsell_0.5.0          reprex_2.0.1           compiler_4.1.2        \n[46] rlang_1.0.2            grid_4.1.2             rstudioapi_0.13.0-9000\n[49] htmlwidgets_1.5.4      crosstalk_1.2.0        labeling_0.4.2        \n[52] rmarkdown_2.14         gtable_0.3.0           DBI_1.1.2             \n[55] R6_2.5.1               lubridate_1.8.0        knitr_1.39            \n[58] fastmap_1.1.0          bit_4.0.4              utf8_1.2.2            \n[61] stringi_1.7.6          parallel_4.1.2         Rcpp_1.0.8.3          \n[64] vctrs_0.4.1            dbplyr_2.1.1           tidyselect_1.1.2      \n[67] xfun_0.31.2"
  },
  {
    "objectID": "Untitled-1.html",
    "href": "Untitled-1.html",
    "title": "Untitled",
    "section": "",
    "text": "players <- read_rds('~/Documents/nfl/data/player_stats/player_stats_2021.rds')\nrosters <- read_rds('~/Documents/nfl/data/roster/roster_2021.rds')\npbp <- read_rds('~/Documents/nfl/data/pbp/play_by_play_2021.rds')\n\n\npbp_roster <- pbp %>%\n  left_join(\n    select(rosters, position, season, gsis_id), by = c('fantasy_id' = 'gsis_id', 'season')\n  )\n\nwr <- pbp_roster %>% \n  filter(position == 'WR') \n\n\nwr_stats_full <- wr %>%\n  mutate(\n    outside_pass = ifelse(pass_location != \"middle\", 1, 0),\n    pass_air_yards = ifelse(is.na(air_yards), 0, air_yards),\n    pass_air_yards = ifelse(ydstogo <= 10, pass_air_yards, NA)\n  ) %>%\n  group_by(receiver_id) %>%\n  summarise(\n    rec = sum(complete_pass),\n    air_yards = mean(pass_air_yards, na.rm = T),\n    yards_per_target = mean(yards_gained, na.rm = T),\n    yards_after_catch = mean(yards_after_catch, na.rm = T),\n    td_rate = mean(pass_touchdown),\n    outside_rec = mean(outside_pass, na.rm = T),\n    dist_from_sticks = mean(pass_air_yards - ydstogo, na.rm = T)\n  ) %>%\n  filter(rec > 50) %>%\n  left_join(\n    pbp %>% \n      count(receiver_id, receiver, posteam) %>% \n      group_by(receiver_id) %>% arrange(-n) %>% \n      mutate(rn = row_number()) %>% \n      filter(rn == 1) %>% \n      select(-n, -rn)) %>%\n  relocate(receiver, .before = rec)\n\nJoining, by = \"receiver_id\"\n\n\n\nwr_stats <- wr_stats_full %>% \n  select(-starts_with('rec'), -posteam) %>% \n  scale()\n\nwr_stats_df <- wr_stats %>% \n  as_tibble()\n\n\nwr_svd <- svd(wr_stats)\nU <- wr_svd[['u']]\nD <- wr_svd[['d']]\nV <- wr_svd[['v']]\nDS <- diag(1/wr_svd[['d']][1:2])\nUS <- as.matrix(U[,1:2])\nVS <- as.matrix(V[,1:2])\nfinal <- VS %*% DS %*% t(US)\n\n\npca_data <- final[1:2, ] %>% MASS::ginv()\n\n\n# Principal components explained variance\nsum(D[[1]], D[[2]])/sum(D)\n\n[1] 0.5453142\n\n\n\npca_data %>% \n  as_tibble() %>% \n  bind_cols(wr_stats_full %>% select(starts_with(\"rec\"), posteam)) %>% \n  ggplot(aes(V1, V2)) +\n  geom_point() +\n  ggrepel::geom_text_repel(aes(label = paste(receiver, posteam)), max.overlaps = 10) +\n  theme_minimal()\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\nUsing compatibility `.name_repair`.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n\n\n\n\n# 43 x 6\n# so k = 6 here\nwr_stats |> dim()\n\n[1] 43  6\n\n\n\nTab 1 - M (n x m) - 43 x 6Tab 2 - U (n x k) - 43 x 6Tab 3 - D (k x k)Tab 4 - V^T (k x m)\n\n\n\nwr_stats |> tibble()\n\n# A tibble: 43 × 1\n   wr_stats[,\"air_yards\"] [,\"yards_per_target\"] [,\"yards_after_cat… [,\"td_rate\"]\n                    <dbl>                 <dbl>               <dbl>        <dbl>\n 1               -1.81                   -1.30               -0.532       -1.57 \n 2               -0.00809                -0.508              -0.743        1.89 \n 3               -0.755                  -0.804              -1.03        -0.510\n 4                0.955                  -0.958              -0.724        0.409\n 5                0.219                  -0.493              -0.466       -0.284\n 6               -0.261                   0.602               0.165        0.328\n 7               -0.766                  -1.06                0.677       -1.03 \n 8                0.910                   0.716              -0.796        2.34 \n 9                0.0861                 -0.173              -1.00         0.938\n10               -1.31                   -0.901              -0.857       -0.836\n# … with 33 more rows, and 1 more variable: wr_stats[5:6] <dbl>"
  },
  {
    "objectID": "posts/nfl-elo-i/elo_adjustments.html",
    "href": "posts/nfl-elo-i/elo_adjustments.html",
    "title": "Untitled",
    "section": "",
    "text": "\\[\n\\underbrace{Elo}_{\\text{current year}} = \\underbrace{Elo}_{\\text{previous year}} \\times \\frac{2}{3} + 1500 \\times \\frac{1}{3}\n\\]\nYou could also use a 4 year rolling mean since the average career is 3.5 years. However, the average player would not make as much difference as key players.\nAdditionally, if you want to value margin of victory that discounts blowouts, e.g. winning by 21 points is not much better than winning by 28\n\\[\n\\text{Margin of Victory Multiplier} = ln(|\\text{PointDiff}| +1) \\times \\frac{2.2}{|\\underbrace{Elo}_{\\text{A}} - \\underbrace{Elo}_{\\text{B}}| \\times 0.001 + 2.2}\n\\]\nIf you want to include turnover and yard differential you could do\nMargin of Victory, Turnover, Yard, Differential Multiplier = MM\n\\[\n\\text{MM} = e^{(\\frac{1}{5}(\\text{Turnover Diff}))} \\times \\text{ ln } (| \\text{PointDiff} | +1) \\times \\frac{2.2}{(|\\underbrace{Elo}_{\\text{A}} - \\underbrace{Elo}_{\\text{B}}| \\times 0.001 + 2.2)}\n\\]\nhttp://schw4rzr0tg0ld.s3-website.eu-central-1.amazonaws.com/blog/2018/08/elo-boosting.html\nThe ELO model uses a scaled logistic function to map differences in strength to probabilities of won and loss. Let \\(Team_{A}\\) be the elo rating for \\(Team_{A}\\), and \\(Team_{B}\\) for \\(Team_{B}\\), and \\(y_{i} = 1\\) if \\(Team_{A}\\) wins, 0.5 if they draw, and 0 if they lose.\n\n\nCode\ndef calculate_predictor(X, beta):\n  return()\n\ndef logistic_function(predictor, scaling_factor = np.log(10)/400):\n    # ELO uses a scaling factor compared of log(10) / 400 compared to the\n  # usual specification\n  1 / (1 + np.exp(-predictor * scaling_factor))\n\ndef logistic_loss(X, y, beta):\n  N = len(y)\n  e_score = logistic_function(calculate_predictor(X, beta))\n  loss = (1 / N) * np.sum(-y * np.log(e_score) - (1 - y) * np.log(1 - e_score))\n\n\n\n\nCode\ncalculate_predictor <- function(X, beta) {\n  # calculate the linear predictor Xbeta\n  colSums(t(X) * beta)\n}\n\nlogistic_function <- function(predictor, scaling_factor = log(10)/400) {\n  # ELO uses a scaling factor compared of log(10) / 400 compared to the\n  # usual specification\n  1 / (1 + exp(-predictor * scaling_factor))\n}\n\nlogistic_loss <- function(X, y, beta) {\n  # number of observations\n  N <- length(y)\n  # expected score, i.e. predicted probability for win/loss\n  e_score <- logistic_function(calculate_predictor(X, beta))\n\n  loss <- (1 / N) * sum(-y * log(e_score) - (1 - y) * log(1 - e_score))\n  return(loss)\n}\n\nupdate_beta <- function(X, y, beta, k) {\n  # number of observations\n  n <- length(y)\n  # expected score, i.e. predicted probability for win/loss\n  e_score <- logistic_function(calculate_predictor(X, beta))\n  # gradient and the current position\n  gradient <- colSums(X * (e_score - y))\n  # update step\n  beta <- beta - k * gradient\n  # print logistic loss\n  #print(logistic_loss(X, y, beta))\n  return(beta)\n}\n\ngradient_descent <- function(batches, beta_init, k, iterations) {\n  # set beta to initial value\n  beta <- beta_init\n  # initalize matrix to store updates\n  beta_history <- matrix(nrow = length(batches),\n                         ncol = length(beta_init))\n  # loop over iterations, aka epochs\n  for (i in 1:iterations) {\n    # loop over mini-batches\n    for (b in 1:length(batches)) {\n      # run update procedure\n      batch <- batches[[b]]\n      beta <- update_beta(batch$X, batch$y, beta, k)\n      beta_history[b, ] <- beta\n    }\n  }\n  return(beta_history)\n}\n\n\n\n\nCode\ndef update_beta(X, y, beta, k):\n  n = len(y)\n  e_score = logistic_function(calculate_predictor(X, beta))\n\n  return()\n\ndef gradient_descent(batches, beta_init, k, iterations):\n  return()\n\n\n\n\n\n\nCode\ndata <- data.frame(\n  t = factor(c(0, 0, 0, 0, 1, 1, 1)),  # period\n  f = factor(c(1, 2, 3, 1, 2, 3, 3)),  # first team\n  s = factor(c(2, 3, 1, 3, 3, 1, 1)),  # second team\n  y = c(1, 1, 0, 1, 1, 0, 1) # win/loss\n  )  \n  \nget_input_variables <- function(data) {\n  design_matrix <-\n    model.matrix(~ as.factor(f) - 1, data) -\n    model.matrix(~ as.factor(s) - 1, data)\n  colnames(design_matrix) <- paste0(\"player_\", seq_len(ncol(design_matrix)))\n  return(list(X = design_matrix, y = data$y))\n}\n\nbatches <- lapply(split(data, data$t), get_input_variables)\n\n\n\n\nCode\n# determine numer of players from data\nP <- max(sapply(batches, function(x) {ncol(x$X)}))\n# run batch gradient descent\nbgd_beta <- gradient_descent(batches, \n                             beta = numeric(P), \n                             k = 1,\n                             iterations = 1)\n# formatting\nbgd_beta_df <- data.frame(bgd_beta)\ncolnames(bgd_beta_df) <- paste0(\"beta_\", seq_len(P))\n\n\n\n\nCode\nelo_ratings <- PlayerRatings::elo(data.frame(apply(data, 2, as.numeric)),\n                                  kfac = 1, init = 0, history = TRUE)\n# formatting\nelo_ratings_df <- data.frame(t(elo_ratings$history[, , \"Rating\"]))\ncolnames(elo_ratings_df) <- paste0(\"beta_\", seq_len(P))\n\n\n\n\nCode\ndata$t <- factor(0)\n# via glm\nsingle_batch <- get_input_variables(data)\nX <- single_batch$X[, -1]  # drop first player for identification\ny <- single_batch$y\nglm_beta <- c(0, coef(glm(y~ -1 + X, family = binomial)))\nglm_beta <- glm_beta * 400 / log(10)  # normalizing factor from ELO\n\n# formatting\nglm_beta_df <- data.frame(t(glm_beta))\ncolnames(glm_beta_df) <- paste0(\"beta_\", seq_len(P))\n\n\n\n\nCode\nbgd_beta_opt <- gradient_descent(list(single_batch), \n                             beta = numeric(P), \n                             k = 1,\n                             iterations = 1000)\n\n# normalize btea_1 to zero\nbgd_beta_opt <- bgd_beta_opt - bgd_beta_opt[1, 1]\n# formatting\nbgd_beta_opt_df <- data.frame(bgd_beta_opt)\ncolnames(bgd_beta_opt_df) <- paste0(\"beta_\", seq_len(P))\n\n\n\n\nCode\n# for ELO\nELO_beta <- gradient_descent(list(single_batch), \n                             beta = numeric(P), \n                             k = 1,\n                             iterations = 1)\nlogistic_loss(single_batch$X, single_batch$y, as.numeric(ELO_beta))\n\n\n\n\nCode\n# for GLM\nGLM_beta <- gradient_descent(list(single_batch), \n                             beta = numeric(P), \n                             k = 1,\n                             iterations = 1000)\nlogistic_loss(single_batch$X, single_batch$y, as.numeric(GLM_beta))"
  },
  {
    "objectID": "posts/nfl-elo-i/elo_notes.html",
    "href": "posts/nfl-elo-i/elo_notes.html",
    "title": "Untitled",
    "section": "",
    "text": "https://andr3w321.com/a-note-on-autocorrelation/ https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/"
  },
  {
    "objectID": "posts/nfl-elo-i/elo_notes.html#elo-functions",
    "href": "posts/nfl-elo-i/elo_notes.html#elo-functions",
    "title": "Untitled",
    "section": "Elo functions",
    "text": "Elo functions\n#yearly_adjustment\n\nelo_current_year = (elo_previous_year * 2/3) + (1500 * 1/3)\n\n#margin of victory\n\\[\\text{Margin of Victory Multiplier} = ln(\\text{WinnerPointDiff} + 1) \\times \\frac{2.2}{\\text{WinnerEloDiff} \\times 0.001 + 2.2}\\]\n\\(\\text{Margin of Victory Multiplier} = LN(ABS(\\text{Point Diff})+1) \\times \\left( \\frac{2.2}{((ELOW-ELOL)*.001+2.2)} \\right)\\)\nWhere PD is the point differential in the game, ELOW is the winning team’s Elo Rating before the game, and ELOL is the losing team’s Elo Rating before the game.\nmargin_of_victory = log(winner_point_diff + 1) * (2.2 / winner_elo_diff * 0.001 + 2.2)"
  },
  {
    "objectID": "posts/nfl-elo-i/index.html",
    "href": "posts/nfl-elo-i/index.html",
    "title": "NFL Elo 1",
    "section": "",
    "text": "For a simple overview of Elo everyone with no record is assigned a default rating of 1500.\nIf two teams rated at 1500 play each other with a \\(K\\) value of 20:\n\nthe winner would end up with a 1510 rating,\nthe loser ending on a 1490 rating.\n\nThe trick of Elo comes when two teams of drastically different ratings play each other.\nFor example when 1600 beats 1400 with a \\(K\\) value of 20, the new ratings are only 1602 and 1398. Not much changed – only a difference of 2 because the favorite won and was expected to win. However, if 1400 beats 1600, the new ratings would be 1418 and 1582. A much bigger change of 18, because a huge upset occurred its likely that the original 1400 and 1600 rating were incorrect.\n\nCode\nimport numpy as np\n\n\nCode\ndef rate_1vs1(p1, p2, k = 20, drawn = False):\n    rp1 = 10 ** (p1/400)\n    rp2 = 10 ** (p2/400)\n    exp_p1 = rp1 / float(rp1 + rp2)\n    exp_p2 = rp2 / float(rp1 + rp2)\n\n    s1, s2 = np.where(drawn == True, [0.5, 0.5], [1, 0])\n\n    new_p1 = p1 + k * (s1 - exp_p1)\n    new_p2 = p2 + k * (s2 - exp_p2)\n    return(new_p1, new_p2)\n    \n# if you need a win probability function\n\ndef win_probability(p1, p2):\n    diff = p1 - p2\n    p = 1 - 1 / (1 + 10 ** (diff / 400.0))\n    return(p)\n\n\nCode\nprint(rate_1vs1(1600, 1400))\nprint(rate_1vs1(1400, 1600))\nprint(win_probability(1600, 1400))\nprint(win_probability(1400, 1600))\n\n\n\nFor mov, fivethirtyeight uses the function ln(abs(mov) + 1)\n\nCode\ndef rate_1vs1(p1, p2, mov = 1, k = 20, drawn = False):\n    \n    k_multiplier = np.where(mov >= 7, 2.0, 1.0)\n    \n    #k_multiplier = 1.0\n    #if mov >= 7: k_multiplier = 2.0\n    \n    rp1 = 10 ** (p1/400)\n    rp2 = 10 ** (p2/400)\n    exp_p1 = rp1 / float(rp1 + rp2)\n    exp_p2 = rp2 / float(rp1 + rp2)\n    \n    s1, s2 = np.where(drawn == True, [0.5, 0.5], [1, 0])\n\n    new_p1 = p1 + k_multiplier * k * (s1 - exp_p1)\n    new_p2 = p2 + k_multiplier * k * (s2 - exp_p2)\n    return(new_p1, new_p2)\n\nAutocorrelation Lets say instead of two neutral teams playing each other on a neutral field we have two mismatched teams playing each other. We would have four possible outcomes\n\nfavorite wins small\nfavorite wins big\nunderdog wins small\nunderdog wins big\n\n\nCode\nprint(rate_1vs1(1550, 1450, 1, 20))\nprint(rate_1vs1(1550, 1450, 14, 20))\nprint(rate_1vs1(1450, 1550, 1, 20))\nprint(rate_1vs1(1450, 1550, 14, 20))\n\nTo have a variable \\(k\\) multiplyer to reward underdog wins and punish good teams for losing to bad teams, we can use he equation (2.2/((ELOW-ELOL)*.001+2.2))\n\\[\n\\frac{2.2}{((ELOW - ELOL) \\times 0.001 + 2.2)}\n\\]\nFor example, if the favorite wins we have \\(2.2/(100 * 0.001 + 2.2) = 0.956\\) and when the underdog wins we have \\(2.2/(-100 * 0.001 + 2.2) = 1.048\\). Our rewritten elo rating function will be:\n\nCode\ndef rate_1vs1(p1, p2, mov = 1, k = 20, drawn = False):\n\n    k_multiplier = np.where(mov >= 7, 2.0, 1.0)\n    corr_m = np.where(mov >= 7, 2.2 / ((p1 - p2)*.001 + 2.2), 1.0)\n\n    rp1 = 10 ** (p1/400)\n    rp2 = 10 ** (p2/400)\n    exp_p1 = rp1 / float(rp1 + rp2)\n    exp_p2 = rp2 / float(rp1 + rp2)\n    \n    s1, s2 = np.where(drawn == True, [0.5, 0.5], [1, 0])\n\n    new_p1 = p1 + k_multiplier * corr_m * k * (s1 - exp_p1)\n    new_p2 = p2 + k_multiplier * corr_m * k * (s2 - exp_p2)\n    return(new_p1, new_p2)\n\n\nCode\n#Our new ratings\n\nprint(rate_1vs1(1550, 1450, 1, 20))\nprint(rate_1vs1(1550, 1450, 14, 20))\nprint(rate_1vs1(1450, 1550, 1, 20))\nprint(rate_1vs1(1450, 1550, 14, 20))"
  },
  {
    "objectID": "posts/nfl-elo-i/elo_autocorr.html",
    "href": "posts/nfl-elo-i/elo_autocorr.html",
    "title": "Untitled",
    "section": "",
    "text": "FiveThirtyEight uses the following formula for their NFL Elo ratings:\n\\[\nR_{Team_{A}}^{k+1} = R_{Team_{A}}^{k} + K \\cdot M(mov) \\cdot A(x) \\cdot (S_{Team_{A} \\; Team_{B}} - \\sigma(x))\n\\]\nwhere \\(mov\\) is the game’s margin of victory, \\(x = R_{Team_{A}}^{k} - R_{Team_{B}}^{k}\\), and\n\\[\n\\Large\n\\begin{align*}\nM(mov) &= \\ln (|mov|+1) \\\\\nA(x) &= \\frac{2.2}{2.2-0.001(-1)^{S_{Team_{A} \\; Team_{B}}} x} \\\\\n& = \\frac{1}{1-(-1)^{S_{Team_{A} \\; Team_{B}}}\\frac{x}{2200}} \\\\\nS_{Team_{A} \\; Team_{B}} &= \\begin{cases} 1 & Team_{A} \\text{ wins} \\\\ 0.5 & Team_{A} \\text{ ties} \\\\ 0 & Team_{A} \\text{ loses}\\end{cases} \\\\\n\\sigma(x) &= \\frac{1}{1+10^{-x/400}}\n\\end{align*}\n\\]\nMy question is the specific justification for the \\[A(x)\\] term:\n\nwhy this form, and\nwhy those numbers.\n\nI’m looking for more than a layman’s explanation, which 538 already offers and is the typical answer elsewhere. (Although this post goes a bit deeper.)"
  },
  {
    "objectID": "posts/nfl-elo-i/elo_autocorr.html#quick-intuition",
    "href": "posts/nfl-elo-i/elo_autocorr.html#quick-intuition",
    "title": "Untitled",
    "section": "Quick intuition",
    "text": "Quick intuition\nSo first you should buy off on the idea that given Team \\[i\\]’s current rating is \\[R_i\\], we should expect its rating after the current game to still be \\[R_i\\]. For example, we shouldn’t ever expect Team \\[i\\]’s rating to increase, because if we did, “we should have rated them higher to begin with”!\nPut in statistical language, this is the statement that we want \\[\\mathbb{E}[R_i^{k+1}\\vert\\text{all prev ratings}] = R_i^k\\], but more on that in a second."
  },
  {
    "objectID": "posts/nfl-elo-i/elo_autocorr.html#the-typical-explanation",
    "href": "posts/nfl-elo-i/elo_autocorr.html#the-typical-explanation",
    "title": "Untitled",
    "section": "The typical explanation",
    "text": "The typical explanation\nSo \\(A(x)\\), which again is\n\\[\n\\begin{equation}\nA(x) = \\frac{1}{1-(-1)^{S_{ij}}\\frac{x}{2200}}\n\\label{eq:autocorr}\n\\end{equation}\n\\]\nis intended to correct for over- or under-inflation of ratings in the model. We see the function is designed so that\n\nIf Team \\[i\\] is the favorite (\\[x>0\\]), a loss is upweighted (\\[A(x)>0\\]) and a win is downweighted (\\[A(x)<0\\]).\nIf Team \\[i\\] is the underdog (\\[x<0\\]), the opposite.\n\nSo this seems like it would correct for over-inflation of rating due to a heavy favorite, and vice-versa for a big underdog.\nHowever, we are left with the questions: why achieve it in this way? And why use the denominator \\[d=2200\\]?"
  },
  {
    "objectID": "posts/nfl-elo-i/elo_autocorr.html#gettin-stats-y",
    "href": "posts/nfl-elo-i/elo_autocorr.html#gettin-stats-y",
    "title": "Untitled",
    "section": "Gettin stats-y",
    "text": "Gettin stats-y\nWe may interpret Elo ratings as a time series where each new rating depends only on the previous rating, plus some “noise.” More specifically, under this interpretation Elo assumes each team has some true rating, its mean, about which it is constantly fluctuating. This is called an autoregressive model, in our case AR(1). We are also assuming the team’s ratings are stationary, meaning (loosely) the mean stays the same over time.\n(By the way, this is a different interpretation than the connection to SGD I wrote about before, but SGD and AR(1) are, in some sense, the same thing.)\nSkipping some details, (I think) this all amounts to needing the next observation in the time series to, in expectation, equal our current observation. That is, \\[\\mathbb{E}[R_i^{k+1}\\vert\\text{prev ratings}] = R_i^k\\].\nThe fact that we’re fretting about a correction term at all arises because of the margin-of-victory \\[M(z)\\] term we are including. This isn’t part of the “classical” Elo rating scheme, and it’s messing everything up! Without it, we have\n\\[\n\\begin{align*}\n\\mathbb{E}[R_k^{k+1}] &= \\mathbb{E}[R_i^k] + \\mathbb{E}[k(S_{ij}-\\sigma(x))] \\\\\n&= R_i^k + k(\\mathbb{E}[S_{ij}] - \\sigma(x)) \\\\\n&= R_i^k\n\\end{align*}\n\\]\nwhich is just fine.\nNow, including a margin-of-victory term \\[M(z)\\], we need\n\\[\n\\mathbb{E}[M(z)\\cdot A(x) \\cdot (S_{ij} - \\sigma(x))] = 0\n\\]\nwhich, computing expectation over all possible game outcomes as encoded in \\[z\\], given \\[R_i^k\\] and \\[R_j^k\\], implies\n\\[\n\\int_{-\\infty}^0 M(z) A(x) (-\\sigma(x)) \\text{Pr}(z) \\ dz + \\int_0^{\\infty} M(z) A(x) (1-\\sigma(x)) \\text{Pr}(z) \\ dz = 0\n\\]\nover some distribution for the margin \\[z\\]. Rearranging, we get\n\\[\n\\begin{equation}\n\\frac{A(x; i \\text{ win})}{A(x; i \\text{ lose})} = \\frac{\\sigma(x)}{1-\\sigma(x)} \\cdot \\frac{\\mathbb{E}[M(z)|i \\text{ lose}]}{\\mathbb{E}[M(z)|i \\text{ wins}]}\n\\label{eq:goal}\n\\end{equation}\n\\]\nand we want some \\[A(x)\\] so this holds for any Elo delta \\[x\\].\nWe should be able to interpolate some function for the expected \\[M(z)\\]’s, and then if we are satisfied with our functional form for \\[A(x)\\], solve for the denominator \\[d\\].\nLet’s try it."
  },
  {
    "objectID": "posts/nfl-elo-i/elo_autocorr.html#in-search-of-d",
    "href": "posts/nfl-elo-i/elo_autocorr.html#in-search-of-d",
    "title": "Untitled",
    "section": "In search of d",
    "text": "In search of d\nWe need to (1) work out the empirical conditional expectations for \\[M(z)\\], then (2) approximate them with functions, and finally (3) solve for \\[d\\] in terms of those functions.\nWe can easily* pull boxscores, Elo ratings, and plot the mean \\[M(z)\\]’s. (*really not that easy, but see code at the bottom of this post.)\nHere’s the expected \\[M(z)\\] given various Elo rating deltas, based on the past 18 NFL seasons.\nNice! So the empirical (conditional) expectations we’re after are both reasonably approximated by linear functions:\n\\[\n\\begin{align*}\n\\mathbb{E}[M(z)|i \\text{ win}] &\\approx \\frac{x}{1000} + 2.2 \\\\\n\\mathbb{E}[M(z)|i \\text{ lose}] &\\approx -\\frac{x}{1000} + 2.2\n\\end{align*}\n\\]\n(Let the reader note: the slope and intercept here are what you might call “eyeball” estimates, although they are quite close to an OLS estimate.)\nReturning to Eq. \\(\\eqref{eq:goal}\\) and using our satisfying functional form for \\[A(x)\\] from Eq. \\(\\eqref{eq:autocorr}\\), we (almost) have\n\\[\n\\begin{align*}\n\\frac{A(x; i \\text{ wins})}{A(x; i \\text{ lose})} &=\n\\frac{\\mathbb{E}[M(z)|i \\text{ wins}]}{\\mathbb{E}[M(z)|i \\text{ lose}]} \\\\\n\\frac{1-x/d}{1+x/d} &\\approx \\frac{-x/1000 + 2.2}{x/1000 + 2.2}\n\\end{align*}\n\\]\nwhich gives \\[d=2200\\]. Voila!\nTo do. To get here, we had to ignore the \\[\\sigma/(1-\\sigma)=10^{x/400}\\] term in Eq. \\(\\eqref{eq:goal}\\). There might be a way to rewrite the expected \\[M(z)\\]’s in a way they still fit the data and cancel this other term out, but if not, I’m not sure."
  },
  {
    "objectID": "posts/nfl-elo-i/elo_autocorr.html#code",
    "href": "posts/nfl-elo-i/elo_autocorr.html#code",
    "title": "Untitled",
    "section": "Code",
    "text": "Code\nFortunately we can make heavy use of 538’s public NFL data.\nFirst some imports to get us running in a Jupyter notebook:\n\nCode\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nThen we load the data, extract just the last few seasons, and run the desired stats:\n\nCode\nbox = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/nfl-elo-game/master/data/nfl_games.csv')\n\n# convert date to datetime\nbox['date'] = pd.to_datetime(box['date'])\n\n# grab since ____ season\nbox = (box[box['date'] > '2003-08-01']\n       .reset_index()\n       .drop(['index'], axis=1)\n       .copy())\n\nn = box.shape[0]\nelodiffs = np.zeros(n)\npdiffs   = np.zeros(n)\nfor i, row in box.iterrows():\n    elodiffs[i] = row['elo1'] - row['elo2'] + (0 if row['neutral']==1 else 65)\n    pdiffs[i]   = row['score1'] - row['score2']\n\nWe could do something like a pdiffs vs. elodiffs plot with Seaborn at this point, perhaps a jointplot like this …\n\nCode\nimport seaborn as sns\nsns.jointplot(elodiffs / 25, pdiffs, ylim=(-50,50), kind='hex')\n\nwhich is quite exciting. But what we’re really interested in is the mean MOV function distributions, which is the much less pretty:\n\nCode\nxs = np.linspace(-300,400,10)\n\ndef g1(x):\n    return x/1000) + 2.2\ndef g2(x):\n    return -x/1000 + 2.2\n\nfig, ax = plt.subplots(1,1, figsize=(8,6))\n\nax.plot(ed, mw, 'ko-', label='Win')\nax.plot(xs, g1(xs), 'k--')\n\nax.plot(ed, ml, 'co-', label='Loss')\nax.plot(xs, g2(xs), 'c--')\n\nax.set_xlabel(r'$R_i - R_j$', fontsize=16)\nax.set_ylabel(r'Mean $M(z)$', fontsize=16)\nax.legend()\n\nplt.show()"
  },
  {
    "objectID": "posts/nfl-elo-i/college_elo_python.html",
    "href": "posts/nfl-elo-i/college_elo_python.html",
    "title": "Untitled",
    "section": "",
    "text": "Code\n# configure API key\nconfiguration = cfbd.Configuration()\nconfiguration.api_key['Authorization'] = '+TdVQcvhNBRAN948z/lsmZN4ETE9sQ60VyQtobVZ+ARur3G1brcO6FLqhoXAYZw1'\nconfiguration.api_key_prefix['Authorization'] = 'Bearer'\n\n# instantiate a games API instance\napi_config = cfbd.ApiClient(configuration)\ngames_api = cfbd.GamesApi(cfbd.ApiClient(configuration))\n\n\nCode\n#logistic curve function\n\ndef get_expected_score(team_a, team_b):\n    exp = (team_b - team_a) / 400\n    return(1 / (1 + 10**exp))\n\n#team_a_new_elo = team_a_old_elo + K (score for team A - expected score for team A)\n\n\nCode\nprint(get_expected_score(1500, 1500))\nprint(get_expected_score(1500, 1400))\nprint(get_expected_score(1750, 1500))\nprint(get_expected_score(2000, 1500))\n\n\nCode\n#Making adjustments to Elo score based on the outcome of a game. \n# takes home team Elo rating, away team Elo rating, and final scoring margin as parameters and returns the new Elo ratings for the participants. \n\ndef get_new_elos(home_rating, away_rating, margin):\n    k = 25\n\n    # score of 0.5 for a tie\n    home_score = 0.5\n    if margin > 0:\n        # score of 1 for a win\n        home_score = 1\n    elif margin < 0:\n        #score of 0 for a loss\n        home_score = 0\n\n    # get expected home score\n    expected_home_score = get_expected_score(home_rating, away_rating)\n    # multiply difference of actual and expected score by k value and adjust home rating\n    new_home_score = home_rating + k * (home_score - expected_home_score)\n\n    # repeat these steps for the away team\n    # away score is inverse of home score\n    away_score = 1 - home_score\n    expected_away_score = get_expected_score(away_rating, home_rating)\n    new_away_score = away_rating + k * (away_score - expected_away_score)\n\n    # return a tuple\n    return(round(new_home_score), round(new_away_score))\n\n\nCode\n# Again, we're merely finding the difference in actual and expected scores for each team, multiplying them by our predefined K factor of 25, and adding that adjustment to the pregame Elo scores. \n# Finally, we returned a tuple of two values, the first being the new Elo rating for the home team and the second being that of the away team. \n# I should probably also note that the margin value refers to the difference in home and away points (i.e. a positive value is a home win while a negative value is an away win)\n\n# takes a data string and converts it to a datetime object for sorting\ndef date_sort(game):\n    game_date = datetime.datetime.strptime(game['start_date'], \"%Y-%m-%dT%H:%M:%S.000Z\")\n    return(game_date)\n\n# returns an elo rating\ndef elo_sort(team):\n    return(team['elo'])\n\n\nCode\ngames = []\n\nfor year in range(2010, 2022):\n    response = games_api.get_games(year = year)\n    games = [*games, *response]\n\ngames = [dict(\n            start_date = g.start_date,\n            home_team = g.home_team,\n            home_conference = g.home_conference,\n            home_points = g.home_points,\n            away_team = g.away_team,\n            away_conference = g.away_conference,\n            away_points = g.away_points\n            ) for g in games if g.home_points is not None and g.away_points is not None]\n\ngames.sort(key = date_sort)\n\n\nCode\n# We can now loop through each game to calculate current Elo ratings for each team. \n# We will also be tracking pregame and postgame Elo ratings for each game for each team so that we can track team trends over time as well as look at how game outcomes affect team ratings.\n\n# dict object to hold current Elo rating for each team\nteams = dict()\n\n# loop through games in order\nfor game in games:\n\n    # get current rating for home team\n    if game['home_team'] in teams:\n        home_elo = teams[game['home_team']]\n    elif game['home_conference'] is not None:\n        # if no rating, set initial rating to 1500 for FBS teams\n        home_elo = 1500\n    else:\n        # otherwise, set initial rating to 1200 for non-FBS teams\n        home_elo = 1200\n\n    # get current rating for away team\n    if game['away_team'] in teams:\n        away_elo = teams[game['away_team']]\n    elif game['away_conference'] is not None:\n        # if no rating, set initial rating to 1500 for FBS teams\n        away_elo = 1500\n    else:\n        # otherwise, set initial rating to 1200 for non-FBS teams\n        away_elo = 1200\n\n    # calculate score margin from game\n    margin = game['home_points'] - game['away_points']\n\n    # get new elo ratings\n    new_elos = get_new_elos(home_elo, away_elo, margin)\n\n    # set pregame elos on game dict\n    game['pregame_home_elo'] = home_elo\n    game['pregame_away_elo'] = away_elo\n\n    # set postgame elos on game dict\n    game['postgame_home_elo'] = new_elos[0]\n    game['postgame_away_elo'] = new_elos[1]\n\n    # set current elo values in teams dict\n    teams[game['home_team']] = new_elos[0]\n    teams[game['away_team']] = new_elos[1]\n\n\nCode\n# For non-FBS teams, we'll set an initial rating of 1200.\n\n#convert our team dict (which holds the current rating for each team) into a list ordered by rating from highest to lowest\n\nend_elos = [dict(team = key, elo = teams[key]) for key in teams]\nend_elos.sort(key = elo_sort, reverse = True)\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# This is the styling I use. Check out other themes here: https://matplotlib.org/3.2.1/gallery/style_sheets/style_sheets_reference.html\nplt.style.use('fivethirtyeight')\n\n# Graph sizing\nplt.rcParams[\"figure.figsize\"] = [20,10]\n\n\nCode\ndef generate_chart(team):\n    team_games = []\n    for game in games:\n        if game['home_team'] == team:\n            team_games.append(dict(start_date = game['start_date'], elo = game['postgame_home_elo']))\n\n        if game['away_team'] == team:\n            team_games.append(dict(start_date = game['start_date'], elo = game['postgame_away_elo']))\n\n    df = pd.DataFrame.from_records(team_games)\n\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['elo'])\n\n    ax.set(xlabel = 'Game No.', ylabel = 'Elo Rating',\n           title = \"Historical Elo Rating - {0}\".format(team))\n\n    plt.show()\n\n\nCode\ngenerate_chart('Michigan')"
  },
  {
    "objectID": "posts/nfl-elo-i/elo_vs_spread.html",
    "href": "posts/nfl-elo-i/elo_vs_spread.html",
    "title": "Untitled",
    "section": "",
    "text": "Code\nurl = \"https://github.com/fivethirtyeight/nfl-elo-game/raw/master/data/nfl_games.csv\"\ndf = pd.read_csv(url)\nprint(df.shape)\ndf.head(5)\n\n\nCode\n# create the spread column. This is just point diff\n\ndf['spread'] = df.score1 - df.score2\nprint(\"Total teams in the dataset:\", len(df.team1.unique()))\nprint(\"Total teams after 1990:\", len(df[df.season > 1990].team1.unique()))\n\n\nCode\ndef color_coded_hist(x, **kwargs):\n    \"\"\"Color the negative and positive bins differently.\n\n    Color scheme from http://colorbrewer2.org/    \n    \"\"\"\n    hist_bins = [-75, -42, -35, -28, -21, -14, -7, 0, 7, 14, 21, 28, 35, 42, 75]\n    __, __, patches = plt.hist(x, density=True, bins=hist_bins, color=\"#f1a340\")\n    # Purple for positive spread:\n    i = next(i for i, val in enumerate(hist_bins) if val == 0)\n    for p in patches[i:]:\n        p.set_facecolor(\"#998ec3\")\n        \n\ndef best_norm(x, **kwargs):\n    \"\"\"Plot the best normal fit for the data.\"\"\"\n    mu, std = stats.norm.fit(x)\n    # Plot the PDF.\n    xmin, xmax = min(x), max(x)\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k--', alpha = 0.6)\n\n\nCode\n# Plot all of the teams' spreads since 1990\n\ng = sns.FacetGrid(\n    df[df.season > 1990].sort_values('team1'), \n    col = \"team1\", col_wrap = 5, height = 2)\n\ng = (g\n.map(color_coded_hist, \"spread\")\n.map(best_norm, \"spread\")\n.set_titles(\"{col_name}\")\n.set_axis_labels(\"spread\", \"density\"))\n\nmsg = \"Histograms of spread (at home) since 1990. Normal approximation is dashed line.\"\nplt.suptitle(msg, y = 1.025, fontsize = 14)\n\n\nCode\n# Show some of the values\nprint(\"~ Overall ~\")\n\nprint(\n    df[\n        df.neutral == 0][\n            ['spread', 'result1']].mean())\n\nprint(\"\\n~ Past few seasons ~\")\n\nprint(\n    df[\n        (df.neutral == 0) & \n        (df.season > 2011)\n        ]\n    .groupby(\"season\")[[\"spread\", \"result1\"]]\n    .mean()\n)\n\n\nCode\n# Create a data frame with lagged rolling means and combine with the existing data\nrolling_avg = (\n    df.groupby(\"season\")[[\"spread\", \"result1\"]]\n    .mean()\n    .rolling(3, min_periods = 1)\n    .mean()\n    .shift(1)\n)\n\nrolling_avg[\"win_pct_advantage\"] = rolling_avg.result1 - 0.5\nrolling_avg[\"spread_advantage\"] = rolling_avg.spread\nrolling_avg = rolling_avg.drop(columns = [\"result1\", \"spread\"])\n\n\nCode\n# Plot everything\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (14.0, 4.0)\nf, ax = plt.subplots(1, 2)\n\nsns.lineplot(x = \"season\", y = \"spread\", data = df[df.neutral == 0], ax = ax[0])\nax[0].plot(rolling_avg.index, rolling_avg.spread_advantage, 'k:')\nax[0].set_ylabel(\"home spread\")\n\nsns.lineplot(x = \"season\", y = \"result1\", data = df[df.neutral == 0], ax = ax[1])\nax[1].plot(rolling_avg.index, rolling_avg.win_pct_advantage + .5, 'k:')\nax[1].set_ylabel(\"home win percent\")\n\nplt.suptitle(\n    'Central tendency and confidence interval of spread (left), '\n    'win percent (right) by season — lagged moving average is dotted line',\n    fontsize = 14)\n\n\nCode\n#Convert from game to team format\n\nhome_games = df[\n    [\"date\", \"season\", \"neutral\", \"playoff\", \"team1\", \"team2\", \"result1\", \"spread\"]\n][df.season > 1990]\n\nhome_games[\"home\"] = 1\n\n# Now swap the teams for \"away\"\naway_games = home_games.rename(columns = {\"team1\": \"team2\", \"team2\": \"team1\"})\naway_games[\"home\"] = 1 - home_games.home\n\n# Remember to switch the meaning of the winning and spread columns too\naway_games[\"result1\"] = 1 - home_games.result1\naway_games[\"spread\"] = -home_games.spread\n\nby_team = pd.concat(\n    [home_games, away_games], ignore_index = True).sort_values(\n    by=[\"season\", \"team1\", \"date\"]\n)\n\n\nCode\nfrom abc import ABC, abstractmethod\nfrom collections import namedtuple\n\nclass Updater(ABC):\n    def __init__(self, *hyperparameters):\n        # Form of lookup:\n        # {season: {team: [{week1 data}, {week2 data}, ... {weekN data}]}}\n        self.lookup = {}\n        self.Params = namedtuple('Params', ['date'] + list(hyperparameters))\n        \n    def iterrows(self):\n        for season, teams in self.lookup.items():\n            for team, results in teams.items():\n                for row in results:\n                    yield dict(season=season, team1=team, **row._asdict())\n        \n    def get_rows(self):\n        return [r for r in self.iterrows()]\n    \n    @abstractmethod\n    def revert_to_mean(self, season, team, keep=.3, n_obs=8):\n        pass\n    \n    @abstractmethod\n    def update(self, row):\n        pass\n\n\nCode\nclass BernoulliUpdater(Updater):\n    def __init__(self):\n        super().__init__('alpha1', 'beta1')\n        \n    def revert_to_mean(self, season, team, keep=.8, n_obs=4):\n        # default\n        alpha = beta = 1 + n_obs * .5\n        # or use existing data        \n        if season in self.lookup and team in self.lookup[season]:\n            last_entry = self.lookup[season][team].pop()\n            date, alpha0, beta0 = last_entry\n            p = alpha0 / (alpha0 + beta0)\n            alpha = 1 + n_obs * (keep * p + (1 - keep) * .5)\n            beta = 1 + n_obs * (keep * (1 - p) + (1 - keep) * .5)\n            # push back the reverted value to the list\n            self.lookup[season][team].append(self.Params(date, alpha, beta))\n        return alpha, beta\n            \n    def update(self, row):\n        if row.season not in self.lookup:\n            self.lookup[row.season] = {}\n        if row.team1 not in self.lookup[row.season]:\n            self.lookup[row.season][row.team1] = []\n            alpha, beta = self.revert_to_mean(row.season - 1, row.team1)\n        else:\n            __, alpha, beta = self.lookup[row.season][row.team1][-1]\n        # THE UPDATE STEP:\n        # a' = a + 1 if win else 0\n        # b' = b + 1 if lose\n        if row.result1 == 1:  # Won\n            alpha_beta_next = self.Params(row.date, alpha + 1, beta)\n        elif row.result1 == 0.5:  # Tie\n            alpha_beta_next = self.Params(row.date, alpha + .5, beta + .5)\n        else:  # Lost\n            alpha_beta_next = self.Params(row.date, alpha, beta + 1)\n        self.lookup[row.season][row.team1].append(alpha_beta_next)\n        return alpha, beta\n\n\nbernoulli_updater = BernoulliUpdater()\nfor i, row in by_team.iterrows():\n    bernoulli_updater.update(row)\n\nab = pd.DataFrame(bernoulli_updater.get_rows()).sort_values(['team1','season'])\ng = ab.groupby('team1')\nab = ab.assign(alpha1 = g.alpha1.shift(), beta1=g.beta1.shift())\n\nbernoulli_dataset = (\n    by_team[[c for c in by_team.columns if c != 'spread']]\n    .merge(ab, on=['season', 'date', 'team1'])\n    .reindex(columns=[\n        'season', 'date', 'home', 'neutral', 'playoff',\n        'team1', 'team2', 'result1', 'alpha1', 'beta1'])\n)\n\n\nCode\nbernoulli_dataset[(bernoulli_dataset.season == 1993) & (bernoulli_dataset.team1 == 'PHI')]\n\n\nCode\n#Convert back from \"by-team\" to \"by-game\" format\n\nb = (\n    bernoulli_dataset[['season', 'date', 'team1', 'alpha1', 'beta1']]\n    .rename(columns=dict(team1='team2', alpha1='alpha2', beta1='beta2'))\n    .merge(bernoulli_dataset, on=['season', 'date', 'team2'])\n    .join(\n        rolling_avg[['win_pct_advantage']]\n        .rename(columns={'win_pct_advantage':'home_advantage'})\n        , on='season')\n)\n\nb = (\n    b.assign(\n        pwin = \n        (b.alpha1 + b.beta2 - 1) / (b.alpha1 + b.beta1 + b.alpha2 + b.beta2 - 2)\n        # if at home and not neutral add home advantage\n        + b.home * (1 - b.neutral) * b.home_advantage\n        # if away and not neutral subtract home advantage\n        - (1 - b.home) * (1 - b.neutral) * b.home_advantage\n        ,\n        success = lambda row:  row.pwin.round() == row.result1\n    )\n    .reindex(columns=(\n        list(bernoulli_dataset.columns)\n        + ['alpha2', 'beta2', 'home_advantage', 'pwin', 'success']\n    ))\n)\n\nprint(b.success.mean())\nb.tail()\n\n\nCode\ndef plot_roc(predicted, actual, resolution=100, ax=None):\n    \"\"\"'predicted' and 'actual' are pandas Series.\"\"\"\n    ax = ax or plt.gca()\n    cutoff = np.linspace(0, 1, resolution)\n    total_pos = (actual == 1).sum()\n    total_neg = (actual != 1).sum()\n    true_positive_rate = np.fromiter(\n        map(lambda c: (actual[predicted > c] == 1).sum() / total_pos, cutoff),\n        float)\n    false_positive_rate = np.fromiter(\n        map(lambda c: (actual[predicted > c] != 1).sum() / total_neg, cutoff),\n        float)\n    ax.plot(\n        false_positive_rate, true_positive_rate,\n        linestyle='-', color=sns.color_palette()[0], linewidth=3)\n    ax.set_xlim([0,1])\n    ax.set_ylim([0,1])\n    ax.plot([0,1], [0,1], 'k:')\n    # Area under the curve\n    auc = sum((true_positive_rate[:-1] + true_positive_rate[1:]) / 2\n              * (false_positive_rate[:-1] - false_positive_rate[1:]))\n    ax.set_title('ROC curve. AUC = {:0.3f}'.format(auc), fontsize=14);\n\n\n## Start the actual plot\nplt.rcParams['figure.figsize'] = (15.0, 3.0)\nf, ax = plt.subplots(1, 3)\n\nsummary = b.groupby(['team1', 'season'], as_index=False).success.mean()\n\n# Histogram\nsns.distplot(summary.success, ax=ax[0], bins=np.linspace(0, 1, 11))\nax[0].axvline(0.5, color='k', linestyle=':')\nax[0].set_ylabel(\"frequency count\")\nax[0].set_title('Model accuracy (grouped by team, season)', fontsize=14)\n\n# Time series\nsns.lineplot(x=\"season\", y=\"success\", data=summary, ax=ax[1])\nax[1].set_ylabel(\"Model success rate\")\nax[1].set_title('Accuracy year over year (mean {:0.0%})'.format(b.success.mean()), fontsize=14)\n\n# ROC\nplot_roc(b.pwin, b.result1, resolution=100, ax=ax[2])\n\n\nCode\nclass TUpdater(Updater):\n    def __init__(self):\n        super().__init__('nu1', 'mu1', 'alpha1', 'beta1')\n        \n    def get_mean_beta(self, season):\n        mean_beta = 16**2 / 2  # Default\n        if season in self.lookup:\n            team_sets = self.lookup[season].values()\n            mean_beta = (\n                sum(ts[-1].beta1 for ts in team_sets)\n                / sum(ts[-1].nu1 for ts in team_sets))\n        return mean_beta\n        \n    def revert_to_mean(self, season, team, keep=.5, n_obs=3):\n        mean_beta = self.get_mean_beta(season - 1)  # Default\n        nu, mu, alpha, beta = n_obs, 0, n_obs / 2, mean_beta * n_obs\n        # or use existing data\n        if season in self.lookup and team in self.lookup[season]:\n            last_entry = self.lookup[season][team].pop()\n            date, nu0, mu0, alpha0, beta0 = last_entry\n            mu = keep * mu0\n            beta = nu * (keep * beta0 / nu0 + (1 - keep) * mean_beta)\n            # push back the reverted value to the list\n            self.lookup[season][team].append(self.Params(date, nu, mu, alpha, beta))\n        return nu, mu, alpha, beta\n            \n    def update(self, row):\n        if row.season not in self.lookup:\n            self.lookup[row.season] = {}\n        if row.team1 not in self.lookup[row.season]:\n            self.lookup[row.season][row.team1] = []\n            nu, mu, alpha, beta = self.revert_to_mean(row.season - 1, row.team1)\n        else:\n            __, nu, mu, alpha, beta = self.lookup[row.season][row.team1][-1]\n        # THE UPDATE STEP:\n        delta = row.spread - mu\n        nu_mu_alpha_beta_next = self.Params(\n            row.date,\n            nu + 1,                       # nu' = nu + 1\n            mu + delta / (nu + 1),        # mu' = mu + delta / (nu + 1)\n            alpha + .5,                   # alpha' = alpha + 1/2\n            beta + delta * (mu + delta / (nu + 1)) / 2\n                                          # beta' = beta + delta * mu' / 2\n        )\n        self.lookup[row.season][row.team1].append(nu_mu_alpha_beta_next)\n        return nu, mu, alpha, beta\n\n\nt_updater = TUpdater()\nfor i, row in by_team.iterrows():\n    t_updater.update(row)\n\nnmab = pd.DataFrame(t_updater.get_rows()).sort_values(['team1','season'])\ng = nmab.groupby('team1')\nnmab = nmab.assign(\n    nu1 = g.nu1.shift(),\n    mu1 = g.mu1.shift(),\n    alpha1 = g.alpha1.shift(),\n    beta1=g.beta1.shift())\n\nt_dataset = (\n    by_team[[c for c in by_team.columns if c != 'result1']]\n    .merge(nmab, on=['season', 'date', 'team1'])\n    .reindex(columns=[\n        'season', 'date', 'home', 'neutral', 'playoff',\n        'team1', 'team2', 'spread', 'nu1', 'mu1', 'alpha1', 'beta1'])\n)\n\n\nCode\nt_dataset[(t_dataset.season == 2017) & (t_dataset.team1 == 'PHI')]\n\n\nCode\nt = (\n    t_dataset[['season', 'date', 'team1', 'nu1', 'mu1', 'alpha1', 'beta1']]\n    .rename(columns=dict(\n        team1='team2', nu1='nu2', mu1='mu2', alpha1='alpha2', beta1='beta2'))\n    .merge(t_dataset, on=['season', 'date', 'team2'])\n    .join(\n        rolling_avg[['spread_advantage']]\n        .rename(columns={'spread_advantage':'home_advantage'})\n        , on='season')\n)\n\nt = (\n    t.assign(\n        pspread =\n            (t.nu1 * t.mu1 - t.nu2 * t.mu2) / (t.nu1 + t.nu2)\n            # if at home and not neutral add home advantage\n            + t.home * (1 - t.neutral) * t.home_advantage\n            # if away and not neutral subtract home advantage\n            - (1 - t.home) * (1 - t.neutral) * t.home_advantage\n        ,\n        betaprime =\n            t.beta1 + t.beta2\n            + (t.nu1 * t.nu2) / (t.nu1 + t.nu2)\n            * (t.mu1 + t.mu2)**2 / 2\n        ,\n        pwin = (\n            lambda row: 1 - stats.t.cdf(\n                0,\n                row.nu1 + row.nu2,\n                loc=row.pspread,\n                scale=(\n                    row.betaprime\n                    * (row.nu1 + row.nu2 + 1)\n                    / (row.nu1 + row.nu2) / (row.alpha1 + row.alpha2)\n                )))\n        ,\n        success = lambda row: row.pwin.round() == (row.spread > 0)\n    )\n    .reindex(columns=(\n        list(t_dataset.columns)\n        + ['nu2', 'mu2', 'alpha2', 'beta2', 'home_advantage', 'pspread', 'pwin', 'success']\n    ))\n)\n\nprint(t.success.mean())\nprint(t.shape)\nt.tail()\n\n\nCode\n# use toggle\nss_res = ((t.spread - t.pspread)**2).sum()\nss_tot = ((t.spread - t.spread.mean())**2).sum()\nr_squared = 1 -  ss_res/ ss_tot\nsns.jointplot(\"spread\", \"pspread\", data=t, kind=\"hex\", space=0, color=\"b\", ratio=4)\ntitle = \"Actual spread vs. mean of  distribution. R squared= {:0.0%}\".format(r_squared)\nplt.suptitle(title, x=.45, y=1.01, fontsize=14)\n\n\nCode\n# use toggle\nplt.rcParams['figure.figsize'] = (15.0, 3.0)\nf, ax = plt.subplots(1, 3)\n\nsummary = t.groupby(['team1', 'season'], as_index=False).success.mean()\n\n# Histogram\nsns.distplot(summary.success, ax=ax[0], bins=np.linspace(0, 1, 11))\nax[0].axvline(0.5, color='k', linestyle=':')\nax[0].set_ylabel(\"frequency count\")\nax[0].set_title('Model accuracy (grouped by team, season)', fontsize=14)\n\n# Time series\nsns.lineplot(x=\"season\", y=\"success\", data=summary, ax=ax[1])\nax[1].set_ylabel(\"Model success rate\")\nax[1].set_title('Accuracy year over year (mean {:0.0%})'.format(t.success.mean()), fontsize=14)\n\n# ROC\nplot_roc(t.pwin, t.spread > 0, resolution=100, ax=ax[2])"
  }
]